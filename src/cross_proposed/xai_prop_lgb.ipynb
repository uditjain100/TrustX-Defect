{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sA-Wg1eamX-z"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ✅ FULL UPDATED END-TO-END SCRIPT (Z-CROSS) — FIXED SHAP + BETTER RELIABILITY\n",
        "# Train on one PROMISE dataset, Test on another (PC1–PC4)\n",
        "#\n",
        "# ✅ Supports your selected 12 pairs:\n",
        "#   PC1→PC2, PC1→PC3, PC1→PC4,\n",
        "#   PC2→PC1, PC2→PC3, PC2→PC4,\n",
        "#   PC3→PC1, PC3→PC2, PC3→PC4,\n",
        "#   PC4→PC1, PC4→PC2, PC4→PC3\n",
        "#\n",
        "# ✅ Uses PC1→NASA mapping before alignment\n",
        "# ✅ TRAIN-only: SMOTE + scaler + calibration + threshold tuning\n",
        "# ✅ TEST-only: evaluation + GLR + Hit@k + ECE + ReliabilityScore\n",
        "#\n",
        "# ✅ FIX: SHAP now works even when model is CalibratedClassifierCV\n",
        "#    - calibrated model used for probabilities (ECE/Brier)\n",
        "#    - base estimator unwrapped for TreeSHAP\n",
        "#\n",
        "# ============================================================\n",
        "\n",
        "import warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, random, os, re\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "if not hasattr(np, \"bool\"): np.bool = np.bool_\n",
        "if not hasattr(np, \"int\"):  np.int  = int\n",
        "\n",
        "from typing import Tuple, Optional, List, Dict\n",
        "import inspect\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, brier_score_loss\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "_HAS_XGB = _HAS_LGBM = _HAS_CB = False\n",
        "PRINTED_SHAP_FALLBACK = False\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    _HAS_XGB = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    _HAS_LGBM = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    _HAS_CB = True\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    from scipy.io import arff as _arff\n",
        "except Exception:\n",
        "    _arff = None\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    _HAS_SHAP = True\n",
        "except Exception:\n",
        "    _HAS_SHAP = False\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        ")\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# -----------------------------\n",
        "# Metric-family mapping (PC1-style ↔ PC2/PC3/PC4-style)\n",
        "# -----------------------------\n",
        "PC1_TO_NASA_MAP = {\n",
        "    \"loc\": \"LOC_EXECUTABLE\",\n",
        "    \"lOComment\": \"LOC_COMMENTS\",\n",
        "    \"locCodeAndComment\": \"LOC_CODE_AND_COMMENT\",\n",
        "    \"v(g)\": \"CYCLOMATIC_COMPLEXITY\",\n",
        "    \"iv(G)\": \"DESIGN_COMPLEXITY\",\n",
        "    \"ev(g)\": \"ESSENTIAL_COMPLEXITY\",\n",
        "    # Halstead-ish (PC2/3/4 often have these)\n",
        "    \"D\": \"HALSTEAD_DIFFICULTY\",\n",
        "    \"E\": \"HALSTEAD_EFFORT\",\n",
        "    \"I\": \"HALSTEAD_CONTENT\",\n",
        "    # best-effort matches (may or may not exist depending on PC2/3/4)\n",
        "    \"B\": \"HALSTEAD_ERROR_EST\",\n",
        "}\n",
        "\n",
        "def _norm_raw_key(x: str) -> str:\n",
        "    return str(x).strip().lower()\n",
        "\n",
        "def _build_casefold_map(m: dict) -> dict:\n",
        "    out = {}\n",
        "    for k, v in m.items():\n",
        "        out[_norm_raw_key(k)] = v\n",
        "    return out\n",
        "\n",
        "_PC1_TO_NASA_CASEFOLD = _build_casefold_map(PC1_TO_NASA_MAP)\n",
        "\n",
        "# -----------------------------\n",
        "# Target inference helpers\n",
        "# -----------------------------\n",
        "_KEYWORDS = [\"defect\", \"bug\", \"fault\", \"class\", \"label\", \"target\", \"isdefect\", \"is_defect\"]\n",
        "\n",
        "def _fit_optional_weight(estimator, X, y, sample_weight=None, **kwargs):\n",
        "    \"\"\"Call estimator.fit, passing sample_weight only if supported.\"\"\"\n",
        "    try:\n",
        "        sig = inspect.signature(estimator.fit)\n",
        "        if \"sample_weight\" in sig.parameters and sample_weight is not None:\n",
        "            return estimator.fit(X, y, sample_weight=sample_weight, **kwargs)\n",
        "        else:\n",
        "            return estimator.fit(X, y, **kwargs)\n",
        "    except (TypeError, ValueError):\n",
        "        return estimator.fit(X, y, **kwargs)\n",
        "\n",
        "def _class_balanced_weights(y: np.ndarray) -> np.ndarray:\n",
        "    cls = np.unique(y)\n",
        "    cw = compute_class_weight(class_weight='balanced', classes=cls, y=y)\n",
        "    m = {c:w for c,w in zip(cls, cw)}\n",
        "    return np.vectorize(m.get)(y)\n",
        "\n",
        "def _tune_threshold(y_val: np.ndarray, p_val_cal: np.ndarray) -> float:\n",
        "    grid = np.linspace(0.05, 0.95, 61)  # wider grid for imbalance\n",
        "    best_t, best_f1 = 0.5, -1.0\n",
        "    for t in grid:\n",
        "        pred = (p_val_cal >= t).astype(int)\n",
        "        f1 = f1_score(y_val, pred, zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    return float(best_t)\n",
        "\n",
        "def _safe_decode_col(s: pd.Series) -> pd.Series:\n",
        "    if s.dtype == object:\n",
        "        try:\n",
        "            return s.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, (bytes, bytearray)) else x)\n",
        "        except Exception:\n",
        "            return s\n",
        "    return s\n",
        "\n",
        "def _coerce_binary(series: pd.Series) -> pd.Series:\n",
        "    s = series.copy()\n",
        "    if pd.api.types.is_numeric_dtype(s):\n",
        "        uniq = sorted(pd.unique(s.dropna()))\n",
        "        if set(uniq).issubset({0,1}):\n",
        "            return s.astype(int)\n",
        "        if set(uniq).issubset({-1,1}):\n",
        "            return ((s + 1) // 2).astype(int)\n",
        "        if len(uniq) == 2:\n",
        "            m = {uniq[0]:0, uniq[1]:1}\n",
        "            return s.map(m).astype(int)\n",
        "\n",
        "    sval = s.astype(str).str.strip().str.lower()\n",
        "    mapping = {\n",
        "        \"true\":1, \"false\":0, \"t\":1, \"f\":0,\n",
        "        \"yes\":1, \"no\":0, \"y\":1, \"n\":0,\n",
        "        \"defective\":1, \"non-defective\":0, \"nondefective\":0, \"clean\":0, \"faulty\":1,\n",
        "        \"positive\":1, \"negative\":0, \"pos\":1, \"neg\":0,\n",
        "        \"1\":1, \"0\":0\n",
        "    }\n",
        "    mapped = sval.map(mapping)\n",
        "    if not mapped.isna().all():\n",
        "        if mapped.isna().any():\n",
        "            u = list(pd.unique(sval))\n",
        "            if len(u) == 2:\n",
        "                m = {u[0]:0, u[1]:1}\n",
        "                mapped = sval.map(m)\n",
        "        if mapped.isna().any():\n",
        "            raise ValueError(\"Could not fully coerce target to binary.\")\n",
        "        return mapped.astype(int)\n",
        "\n",
        "    u = list(pd.unique(sval))\n",
        "    if len(u) == 2:\n",
        "        return sval.map({u[0]:0, u[1]:1}).astype(int)\n",
        "    raise ValueError(\"Column is not binary or has unexpected labels.\")\n",
        "\n",
        "def _is_binary_col(s: pd.Series) -> bool:\n",
        "    try:\n",
        "        _ = _coerce_binary(s)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _score_target_candidate(colname: str, s: pd.Series, pos_index: int, total_cols: int) -> float:\n",
        "    name = colname.lower()\n",
        "    score = 0.0\n",
        "    if any(k in name for k in _KEYWORDS): score += 2.0\n",
        "    if pos_index == total_cols - 1:       score += 1.0\n",
        "    if _is_binary_col(s):                 score += 2.0\n",
        "    try:\n",
        "        y = _coerce_binary(s)\n",
        "        p = y.mean()\n",
        "        if 0.05 <= p <= 0.95: score += 1.0\n",
        "        else:                  score += 0.5\n",
        "    except Exception:\n",
        "        pass\n",
        "    return score\n",
        "\n",
        "def infer_target_and_features(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, str]:\n",
        "    for c in df.columns:\n",
        "        df[c] = _safe_decode_col(df[c])\n",
        "\n",
        "    cols = list(df.columns)\n",
        "    scores = []\n",
        "    for i, c in enumerate(cols):\n",
        "        s = df[c]\n",
        "        scores.append((_score_target_candidate(c, s, i, len(cols)), i, c))\n",
        "    scores.sort(reverse=True)\n",
        "\n",
        "    for _, idx, cname in scores:\n",
        "        s = df[cname]\n",
        "        if _is_binary_col(s):\n",
        "            try:\n",
        "                y = _coerce_binary(s)\n",
        "                X = df.drop(columns=[cname]).copy()\n",
        "                print(f\"[Target] Auto-detected column: '{cname}' (pos {idx+1}/{len(cols)})\")\n",
        "                return X, y.astype(int), cname\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "    last = cols[-1]\n",
        "    if _is_binary_col(df[last]):\n",
        "        y = _coerce_binary(df[last])\n",
        "        X = df.iloc[:, :-1].copy()\n",
        "        print(f\"[Target] Fallback to last column: '{last}'\")\n",
        "        return X, y.astype(int), last\n",
        "\n",
        "    for c in cols:\n",
        "        if df[c].nunique(dropna=True) == 2:\n",
        "            y = _coerce_binary(df[c])\n",
        "            X = df.drop(columns=[c]).copy()\n",
        "            print(f\"[Target] Fallback to 2-unique column: '{c}'\")\n",
        "            return X, y.astype(int), c\n",
        "\n",
        "    raise ValueError(\"Could not automatically infer a binary target column.\")\n",
        "\n",
        "def load_dataset_auto(data_path: str) -> pd.DataFrame:\n",
        "    ext = os.path.splitext(data_path)[1].lower()\n",
        "    if ext == \".csv\":\n",
        "        return pd.read_csv(data_path)\n",
        "    elif ext == \".arff\":\n",
        "        if _arff is None:\n",
        "            raise ImportError(\"scipy is required for reading .arff files (scipy.io.arff).\")\n",
        "        data, meta = _arff.loadarff(data_path)\n",
        "        df = pd.DataFrame(data)\n",
        "        for c in df.columns:\n",
        "            df[c] = _safe_decode_col(df[c])\n",
        "        for c in df.columns:\n",
        "            if df[c].dtype == object:\n",
        "                try:\n",
        "                    df[c] = pd.to_numeric(df[c])\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return df\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported file extension: {ext}. Use .csv or .arff\")\n",
        "\n",
        "def load_and_preprocess(data_path: str) -> Tuple[pd.DataFrame, pd.Series, str]:\n",
        "    df = load_dataset_auto(data_path)\n",
        "    X, y, tgt = infer_target_and_features(df)\n",
        "\n",
        "    const_cols = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
        "    if const_cols:\n",
        "        print(\"Dropping constant columns:\", const_cols)\n",
        "        X.drop(columns=const_cols, inplace=True)\n",
        "\n",
        "    if X.isna().sum().any():\n",
        "        print(\"Missing values detected; forward/back fill.\")\n",
        "        X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    else:\n",
        "        print(\"No missing values detected.\")\n",
        "\n",
        "    for c in X.columns:\n",
        "        if X[c].dtype == object:\n",
        "            try:\n",
        "                X[c] = pd.to_numeric(X[c])\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    return X, y.astype(int), tgt\n",
        "\n",
        "# -----------------------------\n",
        "# Models\n",
        "# -----------------------------\n",
        "def make_model(name: str, random_state: int):\n",
        "    n = name.strip().lower()\n",
        "    if n in (\"random_forest\",\"rf\"):\n",
        "        return RandomForestClassifier(\n",
        "            n_estimators=800, max_depth=None, min_samples_leaf=2,\n",
        "            random_state=random_state, n_jobs=-1, class_weight=\"balanced_subsample\", oob_score=False\n",
        "        )\n",
        "    if n in (\"extra_trees\",\"extratrees\",\"et\"):\n",
        "        return ExtraTreesClassifier(\n",
        "            n_estimators=1200, max_depth=None, min_samples_leaf=1,\n",
        "            random_state=random_state, n_jobs=-1, class_weight=\"balanced_subsample\"\n",
        "        )\n",
        "    if n in (\"gradient_boosting\",\"gbrt\",\"gb\"):\n",
        "        return GradientBoostingClassifier(\n",
        "            random_state=random_state, learning_rate=0.05, n_estimators=1000, max_depth=3\n",
        "        )\n",
        "    if n in (\"adaboost\",\"ada\"):\n",
        "        base = DecisionTreeClassifier(max_depth=2, random_state=random_state)\n",
        "        params = dict(n_estimators=1000, learning_rate=0.05, random_state=random_state)\n",
        "        try:\n",
        "            return AdaBoostClassifier(estimator=base, **params)\n",
        "        except TypeError:\n",
        "            return AdaBoostClassifier(base_estimator=base, **params)\n",
        "    if n in (\"xgboost\",\"xgb\"):\n",
        "        if not _HAS_XGB:\n",
        "            print(\"[WARN] xgboost not installed; falling back to RandomForest.\")\n",
        "            return make_model(\"rf\", random_state)\n",
        "        return XGBClassifier(\n",
        "            n_estimators=1500, max_depth=6, learning_rate=0.05,\n",
        "            subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0,\n",
        "            eval_metric=\"logloss\", tree_method=\"hist\", random_state=random_state, n_jobs=-1\n",
        "        )\n",
        "    if n in (\"lightgbm\",\"lgbm\",\"lgb\"):\n",
        "        if not _HAS_LGBM:\n",
        "            print(\"[WARN] lightgbm not installed; falling back to RandomForest.\")\n",
        "            return make_model(\"rf\", random_state)\n",
        "        return LGBMClassifier(\n",
        "            n_estimators=2000, num_leaves=31, learning_rate=0.03,\n",
        "            subsample=0.8, colsample_bytree=0.8, random_state=random_state, n_jobs=-1\n",
        "        )\n",
        "    if n in (\"catboost\",\"cb\"):\n",
        "        if not _HAS_CB:\n",
        "            print(\"[WARN] catboost not installed; falling back to RandomForest.\")\n",
        "            return make_model(\"rf\", random_state)\n",
        "        return CatBoostClassifier(\n",
        "            iterations=1500, depth=6, learning_rate=0.05,\n",
        "            verbose=False, random_seed=random_state, loss_function=\"Logloss\"\n",
        "        )\n",
        "    if n in (\"mlp\",\"nn\",\"pytorch_mlp\"):\n",
        "        return MLPClassifier(\n",
        "            hidden_layer_sizes=(256,128,64), activation=\"relu\", solver=\"adam\",\n",
        "            alpha=3e-4, learning_rate_init=1e-3, max_iter=500,\n",
        "            batch_size='auto', early_stopping=True, n_iter_no_change=25,\n",
        "            random_state=random_state\n",
        "        )\n",
        "    raise ValueError(f\"Unknown model name: {name}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Reliability helpers\n",
        "# -----------------------------\n",
        "def spearman_rank_corr(a: pd.Series, b: pd.Series) -> float:\n",
        "    a, b = a.align(b, join='inner')\n",
        "    av, bv = a.values, b.values\n",
        "    if np.nanstd(av) == 0 and np.nanstd(bv) == 0: return 1.0\n",
        "    if np.nanstd(av) == 0 or np.nanstd(bv) == 0:  return 0.0\n",
        "    return spearmanr(pd.Series(av).rank(), pd.Series(bv).rank(), nan_policy='omit').correlation\n",
        "\n",
        "def bce_loss(p, y):\n",
        "    p = np.clip(p, 1e-8, 1-1e-8)\n",
        "    return -(y*np.log(p) + (1-y)*np.log(1-p))\n",
        "\n",
        "def finite_diff_grad_loss(model, X_eval: np.ndarray, y_eval: np.ndarray, eps_vec: np.ndarray):\n",
        "    n, d = X_eval.shape\n",
        "    grads = np.zeros((n, d), dtype=float)\n",
        "    for j in range(d):\n",
        "        e = np.zeros_like(X_eval); e[:, j] = eps_vec[j]\n",
        "        pp = model.predict_proba(X_eval + e)[:, 1]\n",
        "        pm = model.predict_proba(X_eval - e)[:, 1]\n",
        "        grads[:, j] = (bce_loss(pp, y_eval) - bce_loss(pm, y_eval)) / (2.0 * eps_vec[j] + 1e-12)\n",
        "    return np.abs(grads)\n",
        "\n",
        "def normalize_rows(A: np.ndarray, eps=1e-12):\n",
        "    s = A.sum(axis=1, keepdims=True) + eps\n",
        "    return A / s\n",
        "\n",
        "def expected_calibration_error(y_true, y_prob, n_bins=10):\n",
        "    y_true = np.asarray(y_true).astype(int)\n",
        "    y_prob = np.asarray(y_prob).astype(float)\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    inds = np.digitize(y_prob, bins) - 1\n",
        "    ece = 0.0\n",
        "    for b in range(n_bins):\n",
        "        mask = inds == b\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        conf = y_prob[mask].mean()\n",
        "        acc = y_true[mask].mean()\n",
        "        ece += (np.sum(mask) / len(y_true)) * np.abs(acc - conf)\n",
        "    return ece\n",
        "\n",
        "def rescale01_rho(rho):\n",
        "    return (rho + 1.0) / 2.0 if not np.isnan(rho) else np.nan\n",
        "\n",
        "def _shap_to_posclass_2d(sv):\n",
        "    if isinstance(sv, list):\n",
        "        return sv[1] if len(sv) > 1 else sv[0]\n",
        "    sv = np.asarray(sv)\n",
        "    if sv.ndim == 3:\n",
        "        return sv[..., 1] if sv.shape[-1] >= 2 else sv.mean(axis=-1)\n",
        "    return sv\n",
        "\n",
        "def make_eps_vec_from_train(Xtr: pd.DataFrame, base_eps: float = 1e-3) -> np.ndarray:\n",
        "    std = Xtr.std(axis=0).values.astype(float)\n",
        "    std = np.where(std < 1e-12, 1.0, std)\n",
        "    return base_eps * std\n",
        "\n",
        "# -----------------------------\n",
        "# ✅ NEW: unwrap calibrated model for SHAP\n",
        "# -----------------------------\n",
        "def unwrap_for_shap(model):\n",
        "    \"\"\"\n",
        "    If model is CalibratedClassifierCV, extract a fitted base estimator for SHAP.\n",
        "    Otherwise return model itself.\n",
        "    \"\"\"\n",
        "    if isinstance(model, CalibratedClassifierCV):\n",
        "        try:\n",
        "            cc = model.calibrated_classifiers_[0]\n",
        "            if hasattr(cc, \"estimator\"):\n",
        "                return cc.estimator\n",
        "            if hasattr(cc, \"base_estimator\"):\n",
        "                return cc.base_estimator\n",
        "        except Exception:\n",
        "            return model\n",
        "    return model\n",
        "\n",
        "# -----------------------------\n",
        "# ✅ UPDATED: global_importance uses unwrapped estimator for TreeSHAP\n",
        "# -----------------------------\n",
        "def global_importance(model, X_train: pd.DataFrame, X_eval: pd.DataFrame, y_eval: Optional[pd.Series] = None):\n",
        "    feature_names = X_eval.columns\n",
        "\n",
        "    model_shap = unwrap_for_shap(model)\n",
        "\n",
        "    if isinstance(model_shap, AdaBoostClassifier) and hasattr(model_shap, \"feature_importances_\"):\n",
        "        imp = np.abs(np.asarray(model_shap.feature_importances_, dtype=float))\n",
        "        imp_s = pd.Series(imp, index=feature_names, name=\"mean|FI|\")\n",
        "        sv_abs = np.tile(imp, (len(X_eval), 1))\n",
        "        return imp_s, sv_abs\n",
        "\n",
        "    def _is_supported_tree(m):\n",
        "        name = m.__class__.__name__.lower()\n",
        "        return any(k in name for k in [\"randomforest\", \"extratrees\", \"gradientboost\", \"xgb\", \"lgbm\", \"catboost\"])\n",
        "\n",
        "    if _HAS_SHAP:\n",
        "        try:\n",
        "            bg = shap.sample(X_train, min(256, len(X_train)))\n",
        "            if _is_supported_tree(model_shap):\n",
        "                explainer = shap.TreeExplainer(\n",
        "                    model_shap, data=bg, feature_perturbation=\"interventional\", model_output=\"probability\"\n",
        "                )\n",
        "                sv = explainer.shap_values(X_eval, check_additivity=False)\n",
        "                sv = _shap_to_posclass_2d(sv)\n",
        "                sv_abs = np.abs(np.asarray(sv))\n",
        "                return pd.Series(sv_abs.mean(axis=0), index=feature_names, name=\"mean|SHAP|\"), sv_abs\n",
        "        except Exception as e:\n",
        "            global PRINTED_SHAP_FALLBACK\n",
        "            if not PRINTED_SHAP_FALLBACK:\n",
        "                print(f\"[SHAP] Falling back to permutation importance: {e}\")\n",
        "                PRINTED_SHAP_FALLBACK = True\n",
        "\n",
        "    # Fallback permutation importance\n",
        "    if y_eval is None or len(np.unique(y_eval)) != 2:\n",
        "        y_for_perm = (model.predict_proba(X_eval)[:, 1] >= 0.5).astype(int)\n",
        "        scoring = \"neg_brier_score\"\n",
        "    else:\n",
        "        y_for_perm = y_eval\n",
        "        scoring = \"roc_auc\"\n",
        "\n",
        "    pi = permutation_importance(\n",
        "        model, X_eval, y_for_perm,\n",
        "        scoring=scoring, n_repeats=8, random_state=42, n_jobs=-1\n",
        "    )\n",
        "    imp = np.abs(pi.importances_mean)\n",
        "    imp_s = pd.Series(imp, index=feature_names, name=\"mean|PermImp|\")\n",
        "    sv_abs = np.tile(imp, (len(X_eval), 1))\n",
        "    return imp_s, sv_abs\n",
        "\n",
        "def safe_smote(Xtr: pd.DataFrame, ytr: pd.Series, random_state: int):\n",
        "    try:\n",
        "        minority = int((ytr == 1).sum())\n",
        "        k = min(5, max(1, minority - 1))\n",
        "        return SMOTE(random_state=random_state, k_neighbors=k).fit_resample(Xtr, ytr)\n",
        "    except Exception as e:\n",
        "        print(f\"[SMOTE] skipped: {e}\")\n",
        "        return Xtr, ytr\n",
        "\n",
        "# -----------------------------\n",
        "# Improved evaluation: calibrate + threshold tune for ALL models\n",
        "# -----------------------------\n",
        "def evaluate_once_train_test(model, Xtr_fit, ytr_fit, Xte: pd.DataFrame, yte: pd.Series,\n",
        "                             model_name: str, seed: int,\n",
        "                             calibrate_all: bool = True,\n",
        "                             calib_method: str = \"sigmoid\",\n",
        "                             calib_cv: int = 3):\n",
        "    name = model_name.strip().lower()\n",
        "\n",
        "    scaler = None\n",
        "    Xtr_use, Xte_use = Xtr_fit, Xte\n",
        "    sample_weight = None\n",
        "\n",
        "    if name in (\"mlp\",\"nn\",\"pytorch_mlp\"):\n",
        "        scaler = StandardScaler().fit(Xtr_fit.values.astype(float))\n",
        "        Xtr_use = pd.DataFrame(scaler.transform(Xtr_fit.values.astype(float)), columns=Xtr_fit.columns)\n",
        "        Xte_use = pd.DataFrame(scaler.transform(Xte.values.astype(float)),      columns=Xte.columns)\n",
        "        sample_weight = _class_balanced_weights(np.asarray(ytr_fit).astype(int))\n",
        "\n",
        "    _fit_optional_weight(model, Xtr_use, ytr_fit, sample_weight=sample_weight)\n",
        "\n",
        "    fitted_model = model\n",
        "    if calibrate_all:\n",
        "        try:\n",
        "            fitted_model = CalibratedClassifierCV(model, method=calib_method, cv=calib_cv)\n",
        "            fitted_model.fit(Xtr_use, ytr_fit)\n",
        "        except Exception as e:\n",
        "            print(f\"[Calib] skipped ({calib_method}): {e}\")\n",
        "            fitted_model = model\n",
        "\n",
        "    thresh = 0.5\n",
        "    try:\n",
        "        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "        (i_tr, i_val), = sss.split(Xtr_use, ytr_fit)\n",
        "        Xval, yval = Xtr_use.iloc[i_val], ytr_fit.iloc[i_val]\n",
        "        p_val = fitted_model.predict_proba(Xval)[:, 1]\n",
        "        thresh = _tune_threshold(np.asarray(yval).astype(int), np.asarray(p_val).astype(float))\n",
        "    except Exception as e:\n",
        "        print(f\"[Thr] tuning skipped: {e}\")\n",
        "        thresh = 0.5\n",
        "\n",
        "    proba_te = fitted_model.predict_proba(Xte_use)[:, 1]\n",
        "    pred = (proba_te >= thresh).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"AUC\": roc_auc_score(yte, proba_te) if len(np.unique(yte)) == 2 else np.nan,\n",
        "        \"F1\":  f1_score(yte, pred, zero_division=0),\n",
        "        \"Precision\": precision_score(yte, pred, zero_division=0),\n",
        "        \"Recall\":    recall_score(yte, pred, zero_division=0),\n",
        "        \"Brier\":     brier_score_loss(yte, proba_te),\n",
        "        \"thr\":       float(thresh),\n",
        "    }\n",
        "    return proba_te, metrics, (scaler, Xtr_use, Xte_use, fitted_model)\n",
        "\n",
        "def glr_rhos_for_test(shap_abs_test: np.ndarray, grad_norm: np.ndarray, feature_names):\n",
        "    n = shap_abs_test.shape[0]\n",
        "    rhos = []\n",
        "    for i in range(n):\n",
        "        s = pd.Series(shap_abs_test[i, :], index=feature_names)\n",
        "        g = pd.Series(grad_norm[i, :],      index=feature_names)\n",
        "        rhos.append(float(spearman_rank_corr(s, g)))\n",
        "    return rhos\n",
        "\n",
        "def epsilon_hitk_for_subset(model, Xte_np, yte_np, shap_abs_test, top_k, eps, m_limit, chosen_idx):\n",
        "    hits = []\n",
        "    d = Xte_np.shape[1]\n",
        "    proba = model.predict_proba(Xte_np)[:, 1]\n",
        "    base_loss = bce_loss(proba, yte_np)\n",
        "\n",
        "    for i in chosen_idx:\n",
        "        x = Xte_np[i, :]; y_i = yte_np[i]\n",
        "        shap_vec = shap_abs_test[i, :]\n",
        "        M = min(m_limit, d)\n",
        "        cand_idx = np.argsort(-shap_vec)[:M]\n",
        "        topk_shap_idx = cand_idx[:top_k]\n",
        "\n",
        "        X_batch = np.tile(x, (M, 1))\n",
        "        X_batch[np.arange(M), cand_idx] += eps\n",
        "        p_pert = model.predict_proba(X_batch)[:, 1]\n",
        "        dL = bce_loss(p_pert, np.full(M, y_i)) - base_loss[i]\n",
        "        topk_delta_idx = cand_idx[np.argsort(-dL)[:top_k]]\n",
        "\n",
        "        hits.append(int(len(set(topk_shap_idx).intersection(set(topk_delta_idx))) > 0))\n",
        "    return hits\n",
        "\n",
        "# -----------------------------\n",
        "# Z-CROSS alignment with PC1 mapping\n",
        "# -----------------------------\n",
        "def apply_pc1_to_nasa_mapping(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    rename_dict = {}\n",
        "    for c in X.columns:\n",
        "        key = _norm_raw_key(c)\n",
        "        if key in _PC1_TO_NASA_CASEFOLD:\n",
        "            rename_dict[c] = _PC1_TO_NASA_CASEFOLD[key]\n",
        "    if rename_dict:\n",
        "        X = X.rename(columns=rename_dict).copy()\n",
        "    return X\n",
        "\n",
        "def _norm_colname(c: str) -> str:\n",
        "    return re.sub(r\"[^a-z0-9]+\", \"_\", str(c).strip().lower())\n",
        "\n",
        "def align_train_test(Xtr: pd.DataFrame, Xte: pd.DataFrame, train_name: str, test_name: str, verbose: bool = True):\n",
        "    if \"pc1\" in train_name.lower():\n",
        "        Xtr = apply_pc1_to_nasa_mapping(Xtr)\n",
        "    if \"pc1\" in test_name.lower():\n",
        "        Xte = apply_pc1_to_nasa_mapping(Xte)\n",
        "\n",
        "    tr_map = {_norm_colname(c): c for c in Xtr.columns}\n",
        "    te_map = {_norm_colname(c): c for c in Xte.columns}\n",
        "    common = sorted(set(tr_map.keys()).intersection(set(te_map.keys())))\n",
        "\n",
        "    if len(common) == 0:\n",
        "        raise ValueError(\"No common features between train and test after mapping + normalization.\")\n",
        "\n",
        "    Xtr_aligned = Xtr[[tr_map[k] for k in common]].copy()\n",
        "    Xte_aligned = Xte[[te_map[k] for k in common]].copy()\n",
        "\n",
        "    Xtr_aligned.columns = common\n",
        "    Xte_aligned.columns = common\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"[Align] common_features={len(common)} | drop_train={Xtr.shape[1]-len(common)} | drop_test={Xte.shape[1]-len(common)}\")\n",
        "    return Xtr_aligned, Xte_aligned, common\n",
        "\n",
        "# -----------------------------\n",
        "# Z-CROSS core runner\n",
        "# -----------------------------\n",
        "def train_on_A_test_on_B(\n",
        "    train_path: str,\n",
        "    test_path: str,\n",
        "    model_name: str = \"random_forest\",\n",
        "    rng: int = 42,\n",
        "    top_k: int = 10,\n",
        "    eps: float = 1e-3,\n",
        "    calib_bins: int = 10,\n",
        "    m_limit: int = 20,\n",
        "    max_eps_samples: int = 200,\n",
        "    verbose: bool = True\n",
        "):\n",
        "    np.random.seed(rng); random.seed(rng)\n",
        "\n",
        "    Xtr, ytr, tgt_tr = load_and_preprocess(train_path)\n",
        "    Xte, yte, tgt_te = load_and_preprocess(test_path)\n",
        "\n",
        "    Xtr, Xte, common_cols = align_train_test(\n",
        "        Xtr, Xte,\n",
        "        train_name=os.path.basename(train_path),\n",
        "        test_name=os.path.basename(test_path),\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    feature_names = Xtr.columns.tolist()\n",
        "    eps_vec = make_eps_vec_from_train(Xtr, base_eps=eps)\n",
        "\n",
        "    # TRAIN-only SMOTE\n",
        "    Xtr_fit, ytr_fit = safe_smote(Xtr, ytr, random_state=rng)\n",
        "\n",
        "    base_model = make_model(model_name, random_state=rng)\n",
        "\n",
        "    proba_te, metrics, (_scaler, Xtr_used, Xte_used, fitted_model) = evaluate_once_train_test(\n",
        "        base_model, Xtr_fit, ytr_fit, Xte, yte,\n",
        "        model_name=model_name, seed=rng,\n",
        "        calibrate_all=True,\n",
        "        calib_method=\"sigmoid\",\n",
        "        calib_cv=3\n",
        "    )\n",
        "\n",
        "    model_for_probs = fitted_model\n",
        "    model_for_shap  = unwrap_for_shap(fitted_model)\n",
        "\n",
        "    imp_test, shap_abs_test = global_importance(model_for_shap, Xtr_used, Xte_used, yte)\n",
        "\n",
        "    Xte_np = Xte_used.values.astype(float)\n",
        "    yte_np = yte.values.astype(int)\n",
        "\n",
        "    grad_abs  = finite_diff_grad_loss(model_for_probs, Xte_np, yte_np, eps_vec)\n",
        "    grad_norm = normalize_rows(grad_abs)\n",
        "\n",
        "    glr_rhos = np.array(glr_rhos_for_test(shap_abs_test, grad_norm, feature_names), dtype=float)\n",
        "    GLR_mean = float(np.nanmean(glr_rhos)) if len(glr_rhos) else np.nan\n",
        "\n",
        "    hitk_flags = []\n",
        "    if max_eps_samples > 0:\n",
        "        idxs = np.arange(len(Xte_np))\n",
        "        np.random.shuffle(idxs)\n",
        "        chosen = idxs[:min(max_eps_samples, len(idxs))]\n",
        "        hitk_flags = epsilon_hitk_for_subset(model_for_probs, Xte_np, yte_np, shap_abs_test,\n",
        "                                             top_k, eps, m_limit, chosen)\n",
        "\n",
        "    Hitk = float(np.mean(hitk_flags)) if len(hitk_flags) else np.nan\n",
        "    ECE = float(expected_calibration_error(yte_np, proba_te, n_bins=calib_bins))\n",
        "    ReliabilityScore = float(np.nanmean([rescale01_rho(GLR_mean), Hitk, max(0.0, 1.0 - ECE)]))\n",
        "\n",
        "    summary = pd.DataFrame([{\n",
        "        \"Train\": os.path.basename(train_path),\n",
        "        \"Test\": os.path.basename(test_path),\n",
        "        \"Model\": model_name,\n",
        "        \"Train_Target\": tgt_tr,\n",
        "        \"Test_Target\": tgt_te,\n",
        "        \"n_common_features\": len(common_cols),\n",
        "\n",
        "        \"AUC\": float(metrics[\"AUC\"]),\n",
        "        \"F1\": float(metrics[\"F1\"]),\n",
        "        \"Precision\": float(metrics[\"Precision\"]),\n",
        "        \"Recall\": float(metrics[\"Recall\"]),\n",
        "        \"Brier\": float(metrics[\"Brier\"]),\n",
        "        \"thr\": float(metrics[\"thr\"]),\n",
        "\n",
        "        \"GLR_mean\": GLR_mean,\n",
        "        f\"Hit@{top_k}\": Hitk,\n",
        "        \"ECE\": ECE,\n",
        "        \"ReliabilityScore\": ReliabilityScore,\n",
        "        \"eps_samples_used\": int(min(max_eps_samples, len(Xte_np))),\n",
        "    }]).round(6)\n",
        "\n",
        "    artifacts = {\n",
        "        \"summary\": summary,\n",
        "        \"metrics\": metrics,\n",
        "        \"proba_test\": np.asarray(proba_te, dtype=float),\n",
        "        \"y_test\": yte_np,\n",
        "        \"glr_rhos\": glr_rhos,\n",
        "        \"hitk_flags\": np.asarray(hitk_flags, dtype=int) if len(hitk_flags) else np.array([], dtype=int),\n",
        "        \"importance_test\": imp_test.sort_values(ascending=False),\n",
        "        \"common_features\": common_cols,\n",
        "    }\n",
        "\n",
        "    return summary, artifacts\n",
        "\n",
        "def run_zcross_selected_pairs(\n",
        "    base_path: str,\n",
        "    selected_pairs: List[Tuple[str, str]],\n",
        "    model_name: str = \"random_forest\",\n",
        "    rng: int = 42,\n",
        "    top_k: int = 10,\n",
        "    eps: float = 1e-3,\n",
        "    calib_bins: int = 10,\n",
        "    m_limit: int = 20,\n",
        "    max_eps_samples: int = 200,\n",
        "    save_csv: bool = True,\n",
        "    out_csv: str = None,\n",
        "):\n",
        "    rows = []\n",
        "    artifacts = {}\n",
        "\n",
        "    for tr_file, te_file in selected_pairs:\n",
        "        tr_path = os.path.join(base_path, tr_file)\n",
        "        te_path = os.path.join(base_path, te_file)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*95)\n",
        "        print(f\"[ZCROSS] {tr_file} → {te_file} | model={model_name} | seed={rng}\")\n",
        "        print(\"=\"*95)\n",
        "\n",
        "        summary, art = train_on_A_test_on_B(\n",
        "            tr_path, te_path,\n",
        "            model_name=model_name,\n",
        "            rng=rng,\n",
        "            top_k=top_k,\n",
        "            eps=eps,\n",
        "            calib_bins=calib_bins,\n",
        "            m_limit=m_limit,\n",
        "            max_eps_samples=max_eps_samples,\n",
        "            verbose=True\n",
        "        )\n",
        "\n",
        "        rows.append(summary.iloc[0].to_dict())\n",
        "        artifacts[(tr_file, te_file)] = art\n",
        "\n",
        "    summary_df = pd.DataFrame(rows)\n",
        "\n",
        "    if save_csv:\n",
        "        if out_csv is None:\n",
        "            out_csv = os.path.join(base_path, f\"zcross_{model_name}_summary.csv\")\n",
        "        summary_df.to_csv(out_csv, index=False)\n",
        "        print(f\"\\n[Saved] {out_csv}\")\n",
        "\n",
        "    return summary_df, artifacts\n",
        "\n",
        "import contextlib, io\n",
        "\n",
        "def run_zcross_with_repeats(\n",
        "    base_path,\n",
        "    selected_pairs,\n",
        "    model_name=\"random_forest\",\n",
        "    repeats=5,\n",
        "    seed0=42,\n",
        "\n",
        "    # behavior controls\n",
        "    verbose_epochs=False,     # ✅ no per-epoch printing\n",
        "    verbose_pairs=False,      # ✅ no per-pair printing inside repeats\n",
        "    final_only=True,          # ✅ return only final aggregated per pair\n",
        "    collect_importance=True,  # ✅ needed for “one plot for all epochs”\n",
        "\n",
        "    # saving\n",
        "    save_csv=True,\n",
        "    out_csv=None,\n",
        "\n",
        "    **kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs repeats internally but prints almost nothing.\n",
        "    Returns:\n",
        "      - final_df: one row per pair (mean±std columns)\n",
        "      - raw_df (optional): all repeats (if final_only=False)\n",
        "      - all_pair_importances: dict pair -> list[Series] (if collect_importance=True)\n",
        "    \"\"\"\n",
        "\n",
        "    all_runs = []\n",
        "    all_pair_importances = {pair: [] for pair in selected_pairs}\n",
        "\n",
        "    for r in range(repeats):\n",
        "        seed = seed0 + 101 * r\n",
        "\n",
        "        # suppress prints during each epoch unless user wants it\n",
        "        if verbose_epochs:\n",
        "            print(f\"\\n######## REPEAT {r+1}/{repeats} | seed={seed} ########\")\n",
        "\n",
        "        # If you want to silence the entire inner prints\n",
        "        if not verbose_epochs and not verbose_pairs:\n",
        "            f = io.StringIO()\n",
        "            with contextlib.redirect_stdout(f):\n",
        "                summary_df, artifacts = run_zcross_selected_pairs(\n",
        "                    base_path=base_path,\n",
        "                    selected_pairs=selected_pairs,\n",
        "                    model_name=model_name,\n",
        "                    rng=seed,\n",
        "                    save_csv=False,\n",
        "                    **kwargs\n",
        "                )\n",
        "        else:\n",
        "            # allow printing from inside\n",
        "            summary_df, artifacts = run_zcross_selected_pairs(\n",
        "                base_path=base_path,\n",
        "                selected_pairs=selected_pairs,\n",
        "                model_name=model_name,\n",
        "                rng=seed,\n",
        "                save_csv=False,\n",
        "                **kwargs\n",
        "            )\n",
        "\n",
        "        summary_df[\"repeat\"] = r + 1\n",
        "        all_runs.append(summary_df)\n",
        "\n",
        "        # collect importances for mean-topk plots across repeats\n",
        "        if collect_importance:\n",
        "            for pair in selected_pairs:\n",
        "                art = artifacts.get(pair, None)\n",
        "                if art is None:\n",
        "                    continue\n",
        "\n",
        "                # prefer \"importance_test\" from each run\n",
        "                imp = art.get(\"importance_test\", None)\n",
        "\n",
        "                # fallbacks\n",
        "                if imp is None or (hasattr(imp, \"empty\") and imp.empty):\n",
        "                    imp = art.get(\"mean_importance_test\", None)\n",
        "                if imp is None or (hasattr(imp, \"empty\") and imp.empty):\n",
        "                    imp = art.get(\"top_importance_mean_test\", None)\n",
        "\n",
        "                if imp is None or (hasattr(imp, \"empty\") and imp.empty):\n",
        "                    continue\n",
        "\n",
        "                if not isinstance(imp, pd.Series):\n",
        "                    imp = pd.Series(imp)\n",
        "\n",
        "                all_pair_importances[pair].append(imp)\n",
        "\n",
        "    raw_df = pd.concat(all_runs, ignore_index=True)\n",
        "\n",
        "    # ---------- FINAL ONE ROW PER PAIR ----------\n",
        "    # group by Train/Test/Model\n",
        "    grp_cols = [\"Train\", \"Test\", \"Model\", \"n_common_features\"]\n",
        "\n",
        "    final_df = raw_df.groupby(grp_cols).agg(\n",
        "        AUC=(\"AUC\", \"mean\"),\n",
        "        F1=(\"F1\", \"mean\"),\n",
        "        Precision=(\"Precision\", \"mean\"),\n",
        "        Recall=(\"Recall\", \"mean\"),\n",
        "        Brier=(\"Brier\", \"mean\"),\n",
        "        GLR=(\"GLR_mean\", \"mean\"),\n",
        "        ECE=(\"ECE\", \"mean\"),\n",
        "        ReliabilityScore=(\"ReliabilityScore\", \"mean\"),\n",
        "    ).reset_index()\n",
        "    # nice rounding\n",
        "    final_df = final_df.round(6)\n",
        "\n",
        "    # save only the final per-pair summary\n",
        "    if save_csv:\n",
        "        if out_csv is None:\n",
        "            out_csv = os.path.join(base_path, f\"zcross_{model_name}_final_repeats{repeats}.csv\")\n",
        "        final_df.to_csv(out_csv, index=False)\n",
        "        print(f\"[Saved FINAL summary] {out_csv}\")\n",
        "\n",
        "    if final_only:\n",
        "        return final_df, all_pair_importances\n",
        "    else:\n",
        "        return final_df, raw_df, all_pair_importances\n",
        "\n",
        "\n",
        "def plot_topk(imp: pd.Series, top_k: int = 15, title: str = \"\", save_path: str = None):\n",
        "    \"\"\"\n",
        "    Simple Top-K horizontal bar plot for a pandas Series (importance).\n",
        "    \"\"\"\n",
        "    if imp is None or len(imp) == 0:\n",
        "        print(\"[Plot] Empty importance. Skipping.\")\n",
        "        return\n",
        "\n",
        "    if not isinstance(imp, pd.Series):\n",
        "        imp = pd.Series(imp)\n",
        "\n",
        "    s = imp.dropna().sort_values(ascending=False)\n",
        "    k = min(top_k, len(s))\n",
        "    top = s.head(k)[::-1]\n",
        "\n",
        "    plt.figure(figsize=(10, max(4, 0.5 * k)))\n",
        "    plt.barh(top.index, top.values)\n",
        "    plt.xlabel(\"mean(|importance|)\")\n",
        "    plt.ylabel(\"Feature\")\n",
        "    plt.title(title if title else f\"Top {k} Features\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        print(f\"[Saved] {save_path}\")\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_all_zcross_pairs_mean(\n",
        "    zcross_artifacts: dict,\n",
        "    pairs: list,\n",
        "    top_k: int = 15,\n",
        "    model_name: str = \"\",\n",
        "    only_nonempty: bool = True,\n",
        "    save_dir: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Plots Top-K MEAN importance for every (train,test) pair in pairs.\n",
        "    ✅ Does NOT rerun repeats.\n",
        "    ✅ Uses artifacts already computed.\n",
        "\n",
        "    Expected artifact keys per pair (tries in this order):\n",
        "      1) \"mean_importance_test\"  (best: across repeats/folds)\n",
        "      2) \"importance_test\"       (single-run importance)\n",
        "      3) \"top_importance_mean_test\" (older name)\n",
        "    \"\"\"\n",
        "\n",
        "    for (tr, te) in pairs:\n",
        "        key = (tr, te)\n",
        "        if key not in zcross_artifacts:\n",
        "            print(f\"[SKIP] No artifacts found for {tr} → {te}\")\n",
        "            continue\n",
        "\n",
        "        art = zcross_artifacts[key]\n",
        "\n",
        "        # try multiple possible keys\n",
        "        imp = art.get(\"mean_importance_test\", None)\n",
        "        if imp is None or (hasattr(imp, \"empty\") and imp.empty):\n",
        "            imp = art.get(\"importance_test\", None)\n",
        "        if imp is None or (hasattr(imp, \"empty\") and imp.empty):\n",
        "            imp = art.get(\"top_importance_mean_test\", None)\n",
        "\n",
        "        if imp is None or (hasattr(imp, \"empty\") and imp.empty) or (isinstance(imp, pd.Series) and len(imp) == 0):\n",
        "            if only_nonempty:\n",
        "                print(f\"[SKIP] Empty importance for {tr} → {te}\")\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"[WARN] Empty importance for {tr} → {te}\")\n",
        "                continue\n",
        "\n",
        "        title = f\"{tr} → {te}\"\n",
        "        if model_name:\n",
        "            title += f\" ({model_name})\"\n",
        "        title += \" — mean importance on TEST\"\n",
        "\n",
        "        save_path = None\n",
        "        if save_dir:\n",
        "            os.makedirs(save_dir, exist_ok=True)\n",
        "            safe_name = f\"{tr.replace('.','_')}__TO__{te.replace('.','_')}\"\n",
        "            if model_name:\n",
        "                safe_name += f\"__{model_name}\"\n",
        "            save_path = os.path.join(save_dir, safe_name + \".png\")\n",
        "\n",
        "        plot_topk(imp, top_k=top_k, title=title, save_path=save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/\"\n",
        "\n",
        "SELECTED_PAIRS = [\n",
        "    (\"pc1.arff\",\"pc2.arff\"), (\"pc1.arff\",\"pc3.arff\"), (\"pc1.arff\",\"pc4.arff\"),\n",
        "    (\"pc2.arff\",\"pc1.arff\"), (\"pc2.arff\",\"pc3.arff\"), (\"pc2.arff\",\"pc4.arff\"),\n",
        "    # (\"pc3.arff\",\"pc1.arff\"), (\"pc3.arff\",\"pc2.arff\"), (\"pc3.arff\",\"pc4.arff\"),\n",
        "    # (\"pc4.arff\",\"pc1.arff\"), (\"pc4.arff\",\"pc2.arff\"), (\"pc4.arff\",\"pc3.arff\"),\n",
        "]\n",
        "\n",
        "# ---- Recommended: repeats for stability ----\n",
        "final_df, all_pair_importances = run_zcross_with_repeats(\n",
        "    base_path=BASE_PATH,\n",
        "    selected_pairs=SELECTED_PAIRS,\n",
        "    model_name=\"lightgbm\",\n",
        "    repeats=5,\n",
        "    seed0=42,\n",
        "    verbose_epochs=False,\n",
        "    verbose_pairs=False,\n",
        "    final_only=True,\n",
        "    collect_importance=True,\n",
        "    top_k=10,\n",
        "    eps=1e-3,\n",
        "    calib_bins=10,\n",
        "    m_limit=20,\n",
        "    max_eps_samples=200,\n",
        "    save_csv=True\n",
        ")\n",
        "\n",
        "print(final_df)\n",
        "\n",
        "zcross_artifacts = {}\n",
        "for pair, imp_list in all_pair_importances.items():\n",
        "    if imp_list:\n",
        "        zcross_artifacts[pair] = {\n",
        "            \"mean_importance_test\": pd.concat(imp_list, axis=1).mean(axis=1)\n",
        "        }\n",
        "\n",
        "plot_all_zcross_pairs_mean(\n",
        "    zcross_artifacts=zcross_artifacts,\n",
        "    pairs=SELECTED_PAIRS,\n",
        "    top_k=15,\n",
        "    model_name=\"lightgbm\",\n",
        "    only_nonempty=True,\n",
        "    save_dir=os.path.join(BASE_PATH, \"zcross_plots_mean\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmqp7VS3myXP",
        "outputId": "f5469829-bf11-4594-e17c-72f45db0750e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|===================| 1446/1458 [01:40<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved FINAL summary] /content/zcross_lightgbm_final_repeats5.csv\n",
            "      Train      Test     Model  n_common_features       AUC        F1  \\\n",
            "0  pc1.arff  pc2.arff  lightgbm                 10  0.874505  0.122346   \n",
            "1  pc1.arff  pc3.arff  lightgbm                 10  0.763076  0.297735   \n",
            "2  pc1.arff  pc4.arff  lightgbm                 10  0.774102  0.245958   \n",
            "3  pc2.arff  pc1.arff  lightgbm                 10  0.695553  0.192975   \n",
            "4  pc2.arff  pc3.arff  lightgbm                 36  0.694107  0.040807   \n",
            "5  pc2.arff  pc4.arff  lightgbm                 36  0.530944  0.002210   \n",
            "\n",
            "   Precision    Recall     Brier       GLR       ECE  ReliabilityScore  \n",
            "0   0.087661  0.208696  0.009916  0.220888  0.036727          0.857906  \n",
            "1   0.361270  0.253750  0.100885  0.250483  0.077477          0.849255  \n",
            "2   0.430620  0.173033  0.110542  0.183926  0.080381          0.837194  \n",
            "3   0.256948  0.155844  0.078149  0.103722  0.074985          0.825626  \n",
            "4   0.219118  0.022500  0.103828  0.378647  0.102053          0.862424  \n",
            "5   0.066667  0.001124  0.121158  0.394779  0.119737          0.859217  \n",
            "[Saved] /content/zcross_plots_mean/pc1_arff__TO__pc2_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc1_arff__TO__pc3_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc1_arff__TO__pc4_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc2_arff__TO__pc1_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc2_arff__TO__pc3_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc2_arff__TO__pc4_arff__lightgbm.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = \"/content/\"\n",
        "\n",
        "SELECTED_PAIRS = [\n",
        "    # (\"pc1.arff\",\"pc2.arff\"), (\"pc1.arff\",\"pc3.arff\"), (\"pc1.arff\",\"pc4.arff\"),\n",
        "    # (\"pc2.arff\",\"pc1.arff\"), (\"pc2.arff\",\"pc3.arff\"), (\"pc2.arff\",\"pc4.arff\"),\n",
        "    (\"pc3.arff\",\"pc1.arff\"), (\"pc3.arff\",\"pc2.arff\"), (\"pc3.arff\",\"pc4.arff\"),\n",
        "    (\"pc4.arff\",\"pc1.arff\"), (\"pc4.arff\",\"pc2.arff\"), (\"pc4.arff\",\"pc3.arff\"),\n",
        "]\n",
        "\n",
        "# ---- Recommended: repeats for stability ----\n",
        "final_df, all_pair_importances = run_zcross_with_repeats(\n",
        "    base_path=BASE_PATH,\n",
        "    selected_pairs=SELECTED_PAIRS,\n",
        "    model_name=\"lightgbm\",\n",
        "    repeats=5,\n",
        "    seed0=42,\n",
        "    verbose_epochs=False,\n",
        "    verbose_pairs=False,\n",
        "    final_only=True,\n",
        "    collect_importance=True,\n",
        "    top_k=10,\n",
        "    eps=1e-3,\n",
        "    calib_bins=10,\n",
        "    m_limit=20,\n",
        "    max_eps_samples=200,\n",
        "    save_csv=True\n",
        ")\n",
        "\n",
        "print(final_df)\n",
        "\n",
        "zcross_artifacts = {}\n",
        "for pair, imp_list in all_pair_importances.items():\n",
        "    if imp_list:\n",
        "        zcross_artifacts[pair] = {\n",
        "            \"mean_importance_test\": pd.concat(imp_list, axis=1).mean(axis=1)\n",
        "        }\n",
        "\n",
        "plot_all_zcross_pairs_mean(\n",
        "    zcross_artifacts=zcross_artifacts,\n",
        "    pairs=SELECTED_PAIRS,\n",
        "    top_k=15,\n",
        "    model_name=\"lightgbm\",\n",
        "    only_nonempty=True,\n",
        "    save_dir=os.path.join(BASE_PATH, \"zcross_plots_mean\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOGtSh9_mumN",
        "outputId": "3d1fb3fb-22c5-443c-d0f9-b04af05fde36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|===================| 1562/1563 [02:03<00:00]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Saved FINAL summary] /content/zcross_lightgbm_final_repeats5.csv\n",
            "      Train      Test     Model  n_common_features       AUC        F1  \\\n",
            "0  pc3.arff  pc1.arff  lightgbm                 10  0.799967  0.266768   \n",
            "1  pc3.arff  pc2.arff  lightgbm                 36  0.777147  0.071655   \n",
            "2  pc3.arff  pc4.arff  lightgbm                 37  0.729632  0.124047   \n",
            "3  pc4.arff  pc1.arff  lightgbm                 10  0.744561  0.197015   \n",
            "4  pc4.arff  pc2.arff  lightgbm                 36  0.827323  0.071648   \n",
            "5  pc4.arff  pc3.arff  lightgbm                 37  0.729776  0.143899   \n",
            "\n",
            "   Precision    Recall     Brier       GLR       ECE  ReliabilityScore  \n",
            "0   0.307399  0.236363  0.068123  0.320732  0.037077          0.874430  \n",
            "1   0.051613  0.130435  0.011407  0.124310  0.063462          0.832898  \n",
            "2   0.235644  0.084270  0.119672  0.293922  0.075919          0.857014  \n",
            "3   0.205284  0.189610  0.078594  0.445295  0.043711          0.892979  \n",
            "4   0.038483  0.521739  0.033657  0.080766  0.078956          0.820476  \n",
            "5   0.235302  0.103750  0.104539  0.229513  0.080220          0.844846  \n",
            "[Saved] /content/zcross_plots_mean/pc3_arff__TO__pc1_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc3_arff__TO__pc2_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc3_arff__TO__pc4_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc4_arff__TO__pc1_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc4_arff__TO__pc2_arff__lightgbm.png\n",
            "[Saved] /content/zcross_plots_mean/pc4_arff__TO__pc3_arff__lightgbm.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uDqna3brmwTg"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}