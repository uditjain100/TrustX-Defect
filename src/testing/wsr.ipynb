{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d495222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSR results saved to: ./../../dataset/wsr_results_all_metrics.csv\n",
      "Total rows: 512\n",
      "      Model Dataset         BaselineMetric  ProposedMetric  W_min_scipy  \\\n",
      "0  Adaboost     MC1               AUC mean        AUC mean          0.0   \n",
      "1  Adaboost     MC1                F1 mean         F1 mean          0.0   \n",
      "2  Adaboost     MC1         Precision mean  Precision mean          0.0   \n",
      "3  Adaboost     MC1            Recall mean     Recall mean          0.0   \n",
      "4  Adaboost     MC1  Generalizability mean        GLR mean          0.0   \n",
      "\n",
      "   W_plus  W_minus  W_signed  p_value  significant_0.05  significant_0.01  \\\n",
      "0     0.0     15.0     -15.0   0.0625             False             False   \n",
      "1    15.0      0.0      15.0   0.0625             False             False   \n",
      "2    15.0      0.0      15.0   0.0625             False             False   \n",
      "3     0.0     15.0     -15.0   0.0625             False             False   \n",
      "4     0.0     15.0     -15.0   0.0625             False             False   \n",
      "\n",
      "   effect_size_r  \n",
      "0           -1.0  \n",
      "1            1.0  \n",
      "2            1.0  \n",
      "3           -1.0  \n",
      "4           -1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon, rankdata\n",
    "\n",
    "# ==== INPUT FILE PATHS ====\n",
    "baseline_file = \"./../../dataset/baseline_5fold_values.csv\"\n",
    "proposed_file = \"./../../dataset/proposed_5fold_values.csv\"\n",
    "\n",
    "# ==== READ DATA ====\n",
    "baseline = pd.read_csv(baseline_file)\n",
    "proposed = pd.read_csv(proposed_file)\n",
    "\n",
    "# ==== METRIC PAIRS (compatible metrics only) ====\n",
    "metric_map = {\n",
    "    \"AUC mean\": \"AUC mean\",\n",
    "    \"F1 mean\": \"F1 mean\",\n",
    "    \"Precision mean\": \"Precision mean\",\n",
    "    \"Recall mean\": \"Recall mean\",\n",
    "    \"Generalizability mean\": \"GLR mean\",\n",
    "    \"Concordance mean\": \"Hit@10\",\n",
    "    \"Stability\": \"ECE\",\n",
    "    \"Reliability Index\": \"Reliability Score\",\n",
    "}\n",
    "# \"Train AUC mean\" (baseline) and \"Brier mean\" (proposed) are intentionally excluded\n",
    "\n",
    "# ==== DATASETS & MODELS ====\n",
    "datasets = [d for d in baseline[\"Dataset\"].unique() if d != \"Mean\"]\n",
    "models = baseline[\"Model\"].unique()\n",
    "\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    proposed_model = f\"{model} (Proposed)\"\n",
    "\n",
    "    for dataset in datasets:\n",
    "        for base_metric, prop_metric in metric_map.items():\n",
    "            # Baseline group\n",
    "            b_mask = (\n",
    "                (baseline[\"Model\"] == model)\n",
    "                & (baseline[\"Dataset\"] == dataset)\n",
    "                & (baseline[\"Metric\"] == base_metric)\n",
    "            )\n",
    "            b_group = baseline[b_mask]\n",
    "\n",
    "            # Proposed group\n",
    "            p_mask = (\n",
    "                (proposed[\"Model\"] == proposed_model)\n",
    "                & (proposed[\"Dataset\"] == dataset)\n",
    "                & (proposed[\"Metric\"] == prop_metric)\n",
    "            )\n",
    "            p_group = proposed[p_mask]\n",
    "\n",
    "            # If either side missing, skip\n",
    "            if b_group.empty or p_group.empty:\n",
    "                continue\n",
    "\n",
    "            # Sort by Fold to align values\n",
    "            b_vals = b_group.sort_values(\"Fold\")[\"Value\"].to_numpy()\n",
    "            p_vals = p_group.sort_values(\"Fold\")[\"Value\"].to_numpy()\n",
    "\n",
    "            if b_vals.shape != p_vals.shape:\n",
    "                print(\n",
    "                    \"Shape mismatch for\",\n",
    "                    model, dataset, base_metric, \"vs\", prop_metric,\n",
    "                    b_vals.shape, p_vals.shape\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            diffs = p_vals - b_vals\n",
    "\n",
    "            # Handle all-zero differences\n",
    "            if np.allclose(diffs, 0):\n",
    "                W_min_scipy = np.nan\n",
    "                W_plus = 0.0\n",
    "                W_minus = 0.0\n",
    "                W_signed = 0.0\n",
    "                pval = np.nan\n",
    "                effect_r = 0.0\n",
    "            else:\n",
    "                # Remove zero differences for rank calculations (Wilcoxon logic)\n",
    "                nz_mask = diffs != 0\n",
    "                d = diffs[nz_mask]\n",
    "\n",
    "                # If after removing zeros nothing is left, skip\n",
    "                if d.size == 0:\n",
    "                    W_min_scipy = np.nan\n",
    "                    W_plus = 0.0\n",
    "                    W_minus = 0.0\n",
    "                    W_signed = 0.0\n",
    "                    pval = np.nan\n",
    "                    effect_r = 0.0\n",
    "                else:\n",
    "                    # Ranks of absolute differences\n",
    "                    ranks = rankdata(np.abs(d))\n",
    "                    W_plus = ranks[d > 0].sum()\n",
    "                    W_minus = ranks[d < 0].sum()\n",
    "                    W_signed = W_plus - W_minus\n",
    "\n",
    "                    # SciPy statistic = min(W_plus, W_minus)\n",
    "                    try:\n",
    "                        w_res = wilcoxon(\n",
    "                            p_vals, b_vals,\n",
    "                            alternative=\"two-sided\",\n",
    "                            zero_method=\"wilcox\",\n",
    "                            mode=\"auto\",\n",
    "                        )\n",
    "                        W_min_scipy = float(w_res.statistic)\n",
    "                        pval = float(w_res.pvalue)\n",
    "                    except ValueError as e:\n",
    "                        print(\"Wilcoxon error for\", model, dataset, base_metric, \":\", e)\n",
    "                        W_min_scipy = np.nan\n",
    "                        pval = np.nan\n",
    "\n",
    "                    # Effect size r (matched rank biserial)\n",
    "                    denom = W_plus + W_minus\n",
    "                    if denom == 0:\n",
    "                        effect_r = np.nan\n",
    "                    else:\n",
    "                        effect_r = (W_plus - W_minus) / denom\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Model\": model,\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"BaselineMetric\": base_metric,\n",
    "                    \"ProposedMetric\": prop_metric,\n",
    "                    # Different W variants:\n",
    "                    \"W_min_scipy\": W_min_scipy,     # what SciPy returns (often 0)\n",
    "                    \"W_plus\": W_plus,               # sum of positive ranks\n",
    "                    \"W_minus\": W_minus,             # sum of negative ranks\n",
    "                    \"W_signed\": W_signed,           # W_plus - W_minus\n",
    "                    # p-value & significance\n",
    "                    \"p_value\": pval,\n",
    "                    \"significant_0.05\": bool(\n",
    "                        (pval is not None)\n",
    "                        and (not np.isnan(pval))\n",
    "                        and (pval <= 0.05)\n",
    "                    ),\n",
    "                    \"significant_0.01\": bool(\n",
    "                        (pval is not None)\n",
    "                        and (not np.isnan(pval))\n",
    "                        and (pval <= 0.01)\n",
    "                    ),\n",
    "                    # Effect size\n",
    "                    \"effect_size_r\": effect_r,\n",
    "                }\n",
    "            )\n",
    "\n",
    "# ==== BUILD AND SAVE SUMMARY TABLE ====\n",
    "wsr_df = pd.DataFrame(results)\n",
    "\n",
    "output_file = \"./../../dataset/wsr_results_all_metrics.csv\"\n",
    "wsr_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"WSR results saved to: {output_file}\")\n",
    "print(f\"Total rows: {len(wsr_df)}\")\n",
    "print(wsr_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb3a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
