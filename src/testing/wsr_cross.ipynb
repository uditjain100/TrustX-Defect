{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8e0264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\udit jain\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\udit jain\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\udit jain\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\udit jain\\appdata\\roaming\\python\\python314\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\udit jain\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\udit jain\\appdata\\roaming\\python\\python314\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install scipy pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c0486d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSR cross results saved to: ./../../dataset/wsr_results_cross.csv\n",
      "Total rows: 728\n",
      "      Model Train_dataset Test_dataset BaseMetric ProposedMetric  \\\n",
      "0  adaboost          MEAN         MEAN   AUC_mean            AUC   \n",
      "1  adaboost           pc1          pc2   AUC_mean            AUC   \n",
      "2  adaboost           pc1          pc3   AUC_mean            AUC   \n",
      "3  adaboost           pc1          pc4   AUC_mean            AUC   \n",
      "4  adaboost           pc2          pc1   AUC_mean            AUC   \n",
      "\n",
      "   Baseline_mean  Proposed_mean  Difference  Improvement  N_pairs  \\\n",
      "0       0.766667       0.754418   -0.012249    -0.012249        5   \n",
      "1       0.794000       0.826308    0.032308     0.032308        5   \n",
      "2       0.806000       0.802661   -0.003339    -0.003339        5   \n",
      "3       0.800000       0.771981   -0.028019    -0.028019        5   \n",
      "4       0.678000       0.680820    0.002820     0.002820        5   \n",
      "\n",
      "   W_min_scipy  W_plus  W_minus  W_signed  p_value  p_value_two_sided  \\\n",
      "0          0.0     0.0     15.0     -15.0  1.00000             0.0625   \n",
      "1         15.0    15.0      0.0      15.0  0.03125             0.0625   \n",
      "2          0.0     0.0     15.0     -15.0  1.00000             0.0625   \n",
      "3          0.0     0.0     15.0     -15.0  1.00000             0.0625   \n",
      "4         15.0    15.0      0.0      15.0  0.03125             0.0625   \n",
      "\n",
      "   significant_0.05  significant_0.01  effect_size_r  \n",
      "0             False             False           -1.0  \n",
      "1              True             False            1.0  \n",
      "2             False             False           -1.0  \n",
      "3             False             False           -1.0  \n",
      "4              True             False            1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import wilcoxon, rankdata\n",
    "\n",
    "# ==== INPUT FILE PATHS ====\n",
    "baseline_file = \"./../../dataset/cross_baseline_5fold_values.csv\"\n",
    "proposed_file = \"./../../dataset/cross_proposed_5fold_values.csv\"\n",
    "\n",
    "# ==== READ DATA ====\n",
    "baseline = pd.read_csv(baseline_file).assign(\n",
    "    Train_norm=lambda df: df[\"Train\"].astype(str).str.replace(\".arff\", \"\", regex=False),\n",
    "    Test_norm=lambda df: df[\"Test\"].astype(str).str.replace(\".arff\", \"\", regex=False),\n",
    ")\n",
    "proposed = pd.read_csv(proposed_file).assign(\n",
    "    Train_norm=lambda df: df[\"Train\"].astype(str).str.replace(\".arff\", \"\", regex=False),\n",
    "    Test_norm=lambda df: df[\"Test\"].astype(str).str.replace(\".arff\", \"\", regex=False),\n",
    ")\n",
    "\n",
    "metric_map = {\n",
    "    \"AUC_mean\": \"AUC\",\n",
    "    \"F1_mean\": \"F1\",\n",
    "    \"Precision_mean\": \"Precision\",\n",
    "    \"Recall_mean\": \"Recall\",\n",
    "    \"Generalizability_mean\": \"GLR\",\n",
    "    \"Stability\": \"ECE\",\n",
    "    \"ReliabilityIndex\": \"ReliabilityScore\",\n",
    "}\n",
    "\n",
    "# +1 means higher is better, -1 means lower is better (e.g., ECE)\n",
    "metric_improvement_sign = {\n",
    "    \"AUC_mean\": 1,\n",
    "    \"F1_mean\": 1,\n",
    "    \"Precision_mean\": 1,\n",
    "    \"Recall_mean\": 1,\n",
    "    \"Generalizability_mean\": 1,\n",
    "    \"Stability\": -1,\n",
    "    \"ReliabilityIndex\": 1,\n",
    "}\n",
    "\n",
    "models = sorted(set(baseline[\"Model\"]) & set(proposed[\"Model\"]))\n",
    "results = []\n",
    "\n",
    "for model in models:\n",
    "    base_model = baseline[baseline[\"Model\"] == model]\n",
    "    prop_model = proposed[proposed[\"Model\"] == model]\n",
    "\n",
    "    for base_metric, prop_metric in metric_map.items():\n",
    "        base_rows = base_model[base_model[\"Metric\"] == base_metric]\n",
    "        prop_rows = prop_model[prop_model[\"Metric\"] == prop_metric]\n",
    "\n",
    "        if base_rows.empty or prop_rows.empty:\n",
    "            continue\n",
    "\n",
    "        merged = base_rows.merge(\n",
    "            prop_rows,\n",
    "            on=[\"Train_norm\", \"Test_norm\", \"Fold\"],\n",
    "            suffixes=(\"_baseline\", \"_proposed\"),\n",
    "        )\n",
    "\n",
    "        if merged.empty:\n",
    "            continue\n",
    "\n",
    "        for (train, test), subset in merged.groupby([\"Train_norm\", \"Test_norm\"]):\n",
    "            b_vals = subset[\"Value_baseline\"].to_numpy()\n",
    "            p_vals = subset[\"Value_proposed\"].to_numpy()\n",
    "\n",
    "            if b_vals.size == 0 or p_vals.size == 0:\n",
    "                continue\n",
    "            if b_vals.shape != p_vals.shape:\n",
    "                continue\n",
    "\n",
    "            improve_sign = metric_improvement_sign.get(base_metric, 1)\n",
    "            diffs_raw = p_vals - b_vals\n",
    "            diffs = diffs_raw * improve_sign\n",
    "            n_pairs = len(diffs)\n",
    "            baseline_mean = float(b_vals.mean())\n",
    "            proposed_mean = float(p_vals.mean())\n",
    "            difference = float(proposed_mean - baseline_mean)\n",
    "            improvement = float(difference * improve_sign)\n",
    "\n",
    "            if np.allclose(diffs, 0, equal_nan=True):\n",
    "                W_min_scipy = np.nan\n",
    "                W_plus = 0.0\n",
    "                W_minus = 0.0\n",
    "                W_signed = 0.0\n",
    "                pval_one_sided = np.nan\n",
    "                pval_two_sided = np.nan\n",
    "                effect_r = 0.0\n",
    "            else:\n",
    "                nz_mask = diffs != 0\n",
    "                d = diffs[nz_mask]\n",
    "\n",
    "                if d.size == 0:\n",
    "                    W_min_scipy = np.nan\n",
    "                    W_plus = 0.0\n",
    "                    W_minus = 0.0\n",
    "                    W_signed = 0.0\n",
    "                    pval_one_sided = np.nan\n",
    "                    pval_two_sided = np.nan\n",
    "                    effect_r = 0.0\n",
    "                else:\n",
    "                    ranks = rankdata(np.abs(d))\n",
    "                    W_plus = ranks[d > 0].sum()\n",
    "                    W_minus = ranks[d < 0].sum()\n",
    "                    W_signed = W_plus - W_minus\n",
    "\n",
    "                    try:\n",
    "                        w_res = wilcoxon(\n",
    "                            diffs,\n",
    "                            alternative=\"greater\",\n",
    "                            zero_method=\"wilcox\",\n",
    "                            mode=\"auto\",\n",
    "                        )\n",
    "                        W_min_scipy = float(w_res.statistic)\n",
    "                        pval_one_sided = float(w_res.pvalue)\n",
    "\n",
    "                        w_res_two = wilcoxon(\n",
    "                            diffs,\n",
    "                            alternative=\"two-sided\",\n",
    "                            zero_method=\"wilcox\",\n",
    "                            mode=\"auto\",\n",
    "                        )\n",
    "                        pval_two_sided = float(w_res_two.pvalue)\n",
    "                    except ValueError:\n",
    "                        W_min_scipy = np.nan\n",
    "                        pval_one_sided = np.nan\n",
    "                        pval_two_sided = np.nan\n",
    "\n",
    "                    denom = W_plus + W_minus\n",
    "                    effect_r = (W_signed / denom) if denom != 0 else np.nan\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Model\": model,\n",
    "                    \"Train_dataset\": train,\n",
    "                    \"Test_dataset\": test,\n",
    "                    \"BaseMetric\": base_metric,\n",
    "                    \"ProposedMetric\": prop_metric,\n",
    "                    \"Baseline_mean\": baseline_mean,\n",
    "                    \"Proposed_mean\": proposed_mean,\n",
    "                    \"Difference\": difference,\n",
    "                    \"Improvement\": improvement,\n",
    "                    \"N_pairs\": n_pairs,\n",
    "                    \"W_min_scipy\": W_min_scipy,\n",
    "                    \"W_plus\": W_plus,\n",
    "                    \"W_minus\": W_minus,\n",
    "                    \"W_signed\": W_signed,\n",
    "                    \"p_value\": pval_one_sided,\n",
    "                    \"p_value_two_sided\": pval_two_sided,\n",
    "                    \"significant_0.05\": bool(\n",
    "                        pval_one_sided is not None and not np.isnan(pval_one_sided) and pval_one_sided <= 0.05\n",
    "                    ),\n",
    "                    \"significant_0.01\": bool(\n",
    "                        pval_one_sided is not None and not np.isnan(pval_one_sided) and pval_one_sided <= 0.01\n",
    "                    ),\n",
    "                    \"effect_size_r\": effect_r,\n",
    "                }\n",
    "            )\n",
    "\n",
    "wsr_df = pd.DataFrame(results)\n",
    "\n",
    "output_file = \"./../../dataset/wsr_results_cross.csv\"\n",
    "wsr_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"WSR cross results saved to: {output_file}\")\n",
    "print(f\"Total rows: {len(wsr_df)}\")\n",
    "print(wsr_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
