{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8957b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reliable XAI (Modular, CSV/ARFF, Multi-Model) — Optimized Single Cell\n",
    "import os, random, numpy as np, pandas as pd, shap, matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# ===================== tiny utils =====================\n",
    "def set_seed(s: Optional[int]):\n",
    "    if s is None: return\n",
    "    random.seed(s); np.random.seed(s); os.environ[\"PYTHONHASHSEED\"]=str(s)\n",
    "\n",
    "def drop_constant_and_fillna(X: pd.DataFrame) -> pd.DataFrame:\n",
    "    X = X.copy()\n",
    "    bad = [c for c in X.columns if X[c].nunique(dropna=False) <= 1]\n",
    "    if bad: X.drop(columns=bad, inplace=True)\n",
    "    if X.isna().any().any(): X = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "    return X\n",
    "\n",
    "def to_numeric_features_only(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            try: df[c] = pd.to_numeric(df[c], errors=\"ignore\")\n",
    "            except Exception: pass\n",
    "    return df.select_dtypes(include=[np.number])\n",
    "\n",
    "def _to_float32(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]): df[c] = df[c].astype(np.float32)\n",
    "    return df\n",
    "\n",
    "def _mean_abs_by_feature(sv, n_features: int) -> np.ndarray:\n",
    "    if isinstance(sv, list):\n",
    "        arr = np.asarray(sv[1] if len(sv)>=2 else sv[0])\n",
    "    else:\n",
    "        arr = np.asarray(sv)\n",
    "    if arr.ndim==2 and arr.shape[1]==n_features:\n",
    "        return np.abs(arr).mean(axis=0)\n",
    "    feat_axes = [ax for ax,d in enumerate(arr.shape) if d==n_features]\n",
    "    if feat_axes:\n",
    "        arr = np.moveaxis(arr, feat_axes[0], -1)\n",
    "        return np.abs(arr).mean(axis=tuple(range(arr.ndim-1)))\n",
    "    return np.abs(arr.reshape(-1, n_features)).mean(axis=0)\n",
    "\n",
    "def spearman_rank_corr(a: pd.Series, b: pd.Series) -> float:\n",
    "    a,b = a.align(b, join=\"inner\")\n",
    "    return spearmanr(a.values, b.values).correlation if len(a) else np.nan\n",
    "\n",
    "def rescale01_rho(rho: float) -> float:\n",
    "    return np.nan if np.isnan(rho) else 0.5*(rho+1)\n",
    "\n",
    "def _decode_bytes_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object and len(df[c])>0 and isinstance(df[c].iloc[0], (bytes, bytearray)):\n",
    "            df[c] = df[c].apply(lambda x: x.decode(\"utf-8\", \"ignore\") if isinstance(x, (bytes, bytearray)) else x)\n",
    "    return df\n",
    "\n",
    "def _binarize_target(y: pd.Series) -> pd.Series:\n",
    "    y = y.copy()\n",
    "    if pd.api.types.is_numeric_dtype(y):\n",
    "        uniq = pd.unique(y)\n",
    "        if len(uniq) > 2: return (y.astype(float) > 0).astype(int)\n",
    "        vals = set(pd.unique(y))\n",
    "        if vals.issubset({0,1}): return y.astype(int)\n",
    "        if vals.issubset({1,2}): return (y.astype(int) == 2).astype(int)\n",
    "        return (y.astype(float) > 0).astype(int)\n",
    "    y_ = y.astype(str).str.strip().str.lower()\n",
    "    truthy = {\"true\",\"y\",\"yes\",\"defect\",\"defects\",\"bug\",\"bugs\",\"t\",\"1\"}\n",
    "    falsy  = {\"false\",\"n\",\"no\",\"clean\",\"f\",\"0\"}\n",
    "    return y_.apply(lambda v: 1 if (v in truthy) else (0 if v in falsy else (1 if v not in falsy else 0))).astype(int)\n",
    "\n",
    "def _predict_proba_clipped(model, cols):\n",
    "    def _f(Z):\n",
    "        if isinstance(Z, np.ndarray):\n",
    "            Z = pd.DataFrame(Z, columns=cols)\n",
    "        p = model.predict_proba(Z)[:, 1]\n",
    "        p = np.clip(p, 1e-6, 1-1e-6)\n",
    "        return np.c_[1-p, p]\n",
    "    return _f\n",
    "\n",
    "def _ensure_df(X, cols: List[str]) -> pd.DataFrame:\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X[cols] if list(X.columns) != list(cols) else X\n",
    "    return pd.DataFrame(np.asarray(X), columns=cols)\n",
    "\n",
    "def _kmeans_background(df: pd.DataFrame, k: int, rng=np.random.RandomState(0)) -> pd.DataFrame:\n",
    "    df = _ensure_df(df, list(df.columns))\n",
    "    try:\n",
    "        obj = shap.kmeans(df, k=k)\n",
    "        X = getattr(obj, \"data\", obj)\n",
    "        return _ensure_df(X, list(df.columns))\n",
    "    except Exception:\n",
    "        idx = rng.choice(len(df), size=min(len(df), k), replace=False)\n",
    "        return df.iloc[idx].copy()\n",
    "\n",
    "# ===================== config (optimized defaults) =====================\n",
    "@dataclass\n",
    "class PipelineConfig:\n",
    "    target_col: Optional[str] = None\n",
    "    rng: int = 42\n",
    "    n_splits: int = 5\n",
    "    use_smote: bool = True\n",
    "    smote_k_max: int = 5\n",
    "    # SHAP\n",
    "    shap_background_size: int = 1024\n",
    "    shap_eval_size: int = 6000\n",
    "    shap_nsamples: int = 300\n",
    "    shap_mode: str = \"auto\"      # \"auto\" | \"disable\"\n",
    "    fast_mode: bool = True       # <<< fast path\n",
    "    fast_shap_k: int = 64        # <<< small background for SHAP\n",
    "    # Concordance / Permutation\n",
    "    perm_repeats: int = 15       # <<< reduced repeats\n",
    "    perm_max_features: Optional[int] = None  # if None -> use top-K from SHAP\n",
    "    concordance_top_k: int = 25\n",
    "    # RF / ExtraTrees\n",
    "    rf_n_estimators: int = 800\n",
    "    rf_max_depth: Optional[int] = 12\n",
    "    rf_min_samples_leaf: int = 3\n",
    "    # LR\n",
    "    lr_C: float = 1.0\n",
    "    lr_max_iter: int = 1000\n",
    "\n",
    "# ---- dataset registry rooted at ./../dataset ----\n",
    "BASE_DATA_DIR = \"./../dataset\"\n",
    "DATASET_REGISTRY: Dict[str, Dict[str, Optional[str]]] = {\n",
    "    \"CM1\": {\"path\": os.path.join(BASE_DATA_DIR, \"CM1.csv\"), \"target\": \"Defective\"},\n",
    "    \"PC1\": {\"path\": os.path.join(BASE_DATA_DIR, \"pc1.arff\"), \"target\": None},\n",
    "    \"PC2\": {\"path\": os.path.join(BASE_DATA_DIR, \"pc2.arff\"), \"target\": None},\n",
    "    \"PC3\": {\"path\": os.path.join(BASE_DATA_DIR, \"pc3.arff\"), \"target\": None},\n",
    "    \"PC4\": {\"path\": os.path.join(BASE_DATA_DIR, \"pc4.arff\"), \"target\": None},\n",
    "    \"MC1\": {\"path\": os.path.join(BASE_DATA_DIR, \"mc1.arff\"), \"target\": None},\n",
    "    \"MC2\": {\"path\": os.path.join(BASE_DATA_DIR, \"mc2.arff\"), \"target\": None},\n",
    "    \"MW1\": {\"path\": os.path.join(BASE_DATA_DIR, \"mw1.arff\"), \"target\": None},\n",
    "}\n",
    "TARGET_CANDIDATES = [\"Defective\",\"defective\",\"defects\",\"bug\",\"bugs\",\"class\",\"label\",\"target\",\"c\"]\n",
    "\n",
    "# ===================== dataset loader =====================\n",
    "def _resolve_dataset(dataset: str) -> Tuple[str, Optional[str]]:\n",
    "    if os.path.exists(dataset):  # direct path given\n",
    "        return dataset, None\n",
    "    key = dataset.strip().upper()\n",
    "    if key in DATASET_REGISTRY:\n",
    "        entry = DATASET_REGISTRY[key]\n",
    "        return entry[\"path\"], entry.get(\"target\")\n",
    "    raise FileNotFoundError(f\"Dataset '{dataset}' not found as path or alias. \"\n",
    "                            f\"Known aliases: {list(DATASET_REGISTRY.keys())}\")\n",
    "\n",
    "def _read_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        hdr = df.iloc[0].astype(str).tolist()\n",
    "        if len(set(hdr)) == len(hdr):\n",
    "            df2 = df.iloc[1:].copy(); df2.columns = hdr; return df2\n",
    "        df.columns = [f\"col_{i}\" for i in range(df.shape[1])]\n",
    "        return df\n",
    "\n",
    "def _read_arff(path: str) -> pd.DataFrame:\n",
    "    data, meta = arff.loadarff(path)\n",
    "    df = pd.DataFrame(data)\n",
    "    return _decode_bytes_cols(df)\n",
    "\n",
    "def _auto_find_target(df: pd.DataFrame, hint: Optional[str]) -> str:\n",
    "    if hint and hint in df.columns: return hint\n",
    "    lowmap = {c.lower(): c for c in df.columns}\n",
    "    for k in TARGET_CANDIDATES:\n",
    "        if k.lower() in lowmap: return lowmap[k.lower()]\n",
    "    return df.columns[-1]\n",
    "\n",
    "def load_dataset(dataset: str, target_col: Optional[str]=None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    path, suggested = _resolve_dataset(dataset)\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".csv\": df = _read_csv(path)\n",
    "    elif ext == \".arff\": df = _read_arff(path)\n",
    "    else: raise ValueError(f\"Unsupported file type: {ext} (expected .csv or .arff)\")\n",
    "    tgt = target_col or _auto_find_target(df, suggested)\n",
    "    if tgt not in df.columns:\n",
    "        raise ValueError(f\"Target '{tgt}' not found in {os.path.basename(path)}. Available: {list(df.columns)}\")\n",
    "    y = _binarize_target(df[tgt].copy())\n",
    "    X = to_numeric_features_only(df.drop(columns=[tgt]).copy())\n",
    "    return X, y\n",
    "\n",
    "# ===================== model wrappers =====================\n",
    "class BaseModel:\n",
    "    def __init__(self, cfg: PipelineConfig, random_state: int):\n",
    "        self.cfg = cfg; self._cols: List[str] = []\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series): raise NotImplementedError\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray: raise NotImplementedError\n",
    "    def shap_importance(self, X_bg: pd.DataFrame, X_eval: pd.DataFrame) -> pd.Series:\n",
    "        raise NotImplementedError\n",
    "    def permutation_importance_stable(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:\n",
    "        cols = self._cols; nrep = self.cfg.perm_repeats\n",
    "        rng0 = np.random.RandomState(1)\n",
    "        base_auc = roc_auc_score(y, self.predict_proba(X)[:,1])\n",
    "        rsum = np.zeros(len(cols))\n",
    "        for r in range(nrep):\n",
    "            rng = np.random.RandomState(rng0.randint(1_000_000))\n",
    "            deltas = []\n",
    "            for c in cols:\n",
    "                Xp = X.copy(); Xp[c] = rng.permutation(Xp[c].values)\n",
    "                auc_p = roc_auc_score(y, self.predict_proba(Xp)[:,1])\n",
    "                deltas.append(max(base_auc - auc_p, 0.0))\n",
    "            ranks = pd.Series(deltas, index=cols).rank(pct=True)\n",
    "            rsum += ranks.values\n",
    "        return pd.Series(rsum/nrep, index=cols, name=\"perm_rank\")\n",
    "\n",
    "class TreeModel(BaseModel):\n",
    "    def __init__(self, estimator, cfg: PipelineConfig, random_state: int):\n",
    "        super().__init__(cfg, random_state); self.model = estimator\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        X = _to_float32(X)\n",
    "        self._cols = list(X.columns); self.model.fit(X[self._cols], y); return self\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        X = _ensure_df(X, self._cols); X = _to_float32(X)\n",
    "        return self.model.predict_proba(X[self._cols])\n",
    "\n",
    "    def shap_importance(self, X_bg: pd.DataFrame, X_eval: pd.DataFrame) -> pd.Series:\n",
    "        if getattr(self.cfg, \"shap_mode\", \"auto\") == \"disable\":\n",
    "            return pd.Series(dtype=float)\n",
    "        cols = self._cols; rng = np.random.RandomState(0)\n",
    "\n",
    "        # background (aggressively small in fast_mode)\n",
    "        if self.cfg.fast_mode:\n",
    "            k = min(self.cfg.fast_shap_k, max(1, len(X_bg)))\n",
    "        else:\n",
    "            bg_size = min(len(X_bg), self.cfg.shap_background_size)\n",
    "            k = min(256, max(1, bg_size))\n",
    "        bg = _kmeans_background(_ensure_df(X_bg, cols), k=k, rng=rng)\n",
    "\n",
    "        # eval subset\n",
    "        X_eval = _ensure_df(X_eval, cols)\n",
    "        if len(X_eval) > self.cfg.shap_eval_size:\n",
    "            X_eval = X_eval.iloc[rng.choice(len(X_eval), size=self.cfg.shap_eval_size, replace=False)]\n",
    "\n",
    "        # compute SHAP\n",
    "        try:\n",
    "            expl = shap.TreeExplainer(\n",
    "                self.model,\n",
    "                data=bg,\n",
    "                feature_perturbation=\"tree_path_dependent\",  # FAST\n",
    "                model_output=\"probability\"\n",
    "            )\n",
    "            sv = expl.shap_values(X_eval, check_additivity=False)\n",
    "        except Exception:\n",
    "            f = _predict_proba_clipped(self.model, list(bg.columns))\n",
    "            expl = shap.KernelExplainer(f, bg, link=\"identity\")\n",
    "            ns = self.cfg.shap_nsamples if not self.cfg.fast_mode else max(100, self.cfg.shap_nsamples // 2)\n",
    "            sv = expl.shap_values(X_eval, nsamples=ns)\n",
    "\n",
    "        imp_vec = _mean_abs_by_feature(sv, n_features=len(cols))\n",
    "        return pd.Series(imp_vec, index=cols, name=\"mean|SHAP|\")\n",
    "\n",
    "class LRModel(BaseModel):\n",
    "    def __init__(self, cfg: PipelineConfig, random_state: int):\n",
    "        super().__init__(cfg, random_state)\n",
    "        self.model = LogisticRegression(\n",
    "            C=cfg.lr_C, max_iter=cfg.lr_max_iter, class_weight=\"balanced\",\n",
    "            solver=\"lbfgs\", n_jobs=None, random_state=random_state\n",
    "        )\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series):\n",
    "        X = _to_float32(X); self._cols = list(X.columns); self.model.fit(X[self._cols], y); return self\n",
    "\n",
    "    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:\n",
    "        X = _ensure_df(X, self._cols); X = _to_float32(X)\n",
    "        return self.model.predict_proba(X[self._cols])\n",
    "\n",
    "    def shap_importance(self, X_bg: pd.DataFrame, X_eval: pd.DataFrame) -> pd.Series:\n",
    "        if getattr(self.cfg, \"shap_mode\", \"auto\") == \"disable\":\n",
    "            return pd.Series(dtype=float)\n",
    "        cols = self._cols; rng = np.random.RandomState(0)\n",
    "        bg = _ensure_df(X_bg, cols)\n",
    "        if len(bg) > self.cfg.shap_background_size:\n",
    "            bg = bg.iloc[rng.choice(len(bg), size=self.cfg.shap_background_size, replace=False)]\n",
    "        sample = _ensure_df(X_eval, cols)\n",
    "        if len(sample) > self.cfg.shap_eval_size:\n",
    "            sample = sample.iloc[rng.choice(len(sample), size=self.cfg.shap_eval_size, replace=False)]\n",
    "        try:\n",
    "            expl = shap.LinearExplainer(self.model, bg)\n",
    "            sv = expl.shap_values(sample)\n",
    "        except Exception:\n",
    "            f = _predict_proba_clipped(self.model, cols)\n",
    "            expl = shap.KernelExplainer(f, bg, link=\"identity\")\n",
    "            ns = self.cfg.shap_nsamples if not self.cfg.fast_mode else max(100, self.cfg.shap_nsamples // 2)\n",
    "            sv = expl.shap_values(sample, nsamples=ns)\n",
    "        imp_vec = _mean_abs_by_feature(sv, n_features=len(cols))\n",
    "        return pd.Series(imp_vec, index=cols, name=\"mean|SHAP|\")\n",
    "\n",
    "def build_model(model_name: str, cfg: PipelineConfig, random_state: int) -> BaseModel:\n",
    "    name = model_name.strip().lower()\n",
    "    if name in (\"rf\",\"random_forest\",\"random-forest\"):\n",
    "        est = RandomForestClassifier(\n",
    "            n_estimators=cfg.rf_n_estimators, max_depth=cfg.rf_max_depth,\n",
    "            min_samples_leaf=cfg.rf_min_samples_leaf, random_state=random_state,\n",
    "            n_jobs=-1, class_weight=\"balanced_subsample\", bootstrap=True\n",
    "        ); return TreeModel(est, cfg, random_state)\n",
    "    if name in (\"extra_trees\",\"extratrees\",\"et\"):\n",
    "        est = ExtraTreesClassifier(\n",
    "            n_estimators=cfg.rf_n_estimators, max_depth=cfg.rf_max_depth,\n",
    "            min_samples_leaf=cfg.rf_min_samples_leaf, random_state=random_state,\n",
    "            n_jobs=-1, class_weight=\"balanced_subsample\", bootstrap=False\n",
    "        ); return TreeModel(est, cfg, random_state)\n",
    "    if name in (\"logreg\",\"logistic\",\"logistic_regression\",\"lr\"):\n",
    "        return LRModel(cfg, random_state)\n",
    "    raise ValueError(\"Unknown model. Use: 'random_forest', 'extra_trees', 'logistic_regression'.\")\n",
    "\n",
    "# ===================== reliability k-fold (optimized) =====================\n",
    "def run_reliability_kfold(X: pd.DataFrame, y: pd.Series, cfg: PipelineConfig, model_name: str):\n",
    "    set_seed(cfg.rng); X = _to_float32(drop_constant_and_fillna(X))\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_splits, shuffle=True, random_state=cfg.rng)\n",
    "\n",
    "    tA,tF,tP,tR, A,F,P,R = [],[],[],[], [],[],[], []\n",
    "    expl_te, rows = [], []\n",
    "\n",
    "    for fold,(tr,te) in enumerate(skf.split(X,y),1):\n",
    "        seed = cfg.rng + fold; set_seed(seed)\n",
    "        Xtr,Xte = X.iloc[tr].copy(), X.iloc[te].copy()\n",
    "        ytr,yte = y.iloc[tr].copy(), y.iloc[te].copy()\n",
    "\n",
    "        Xfit,yfit = Xtr,ytr\n",
    "        if cfg.use_smote:\n",
    "            try:\n",
    "                minority = int((ytr==1).sum()); k = min(cfg.smote_k_max, max(1, minority-1))\n",
    "                Xfit,yfit = SMOTE(random_state=seed, k_neighbors=k).fit_resample(Xtr,ytr)\n",
    "            except Exception: pass\n",
    "\n",
    "        model = build_model(model_name, cfg, random_state=seed).fit(Xfit,yfit)\n",
    "\n",
    "        # ---- metrics\n",
    "        p_tr = model.predict_proba(Xtr)[:,1]; yhat_tr = (p_tr>=0.5).astype(int)\n",
    "        tA.append(roc_auc_score(ytr,p_tr)); tF.append(f1_score(ytr,yhat_tr))\n",
    "        tP.append(precision_score(ytr,yhat_tr, zero_division=0)); tR.append(recall_score(ytr,yhat_tr))\n",
    "\n",
    "        p_te = model.predict_proba(Xte)[:,1]; yhat_te = (p_te>=0.5).astype(int)\n",
    "        A.append(roc_auc_score(yte,p_te)); F.append(f1_score(yte,yhat_te))\n",
    "        P.append(precision_score(yte,yhat_te, zero_division=0)); R.append(recall_score(yte,yhat_te))\n",
    "\n",
    "        # ---- SHAP once per fold (test set only for speed)\n",
    "        gte = model.shap_importance(Xfit, Xte)\n",
    "        expl_te.append(gte)\n",
    "\n",
    "        # ultra-cheap proxy for Generalizability in fast_mode\n",
    "        gtr = gte.copy() if cfg.fast_mode else model.shap_importance(Xfit, Xtr)\n",
    "\n",
    "        # ---- Permutation importance only on top features\n",
    "        K = min(cfg.concordance_top_k, len(gte))\n",
    "        top_shap = gte.sort_values(ascending=False).head(K).index.tolist()\n",
    "        perm_pool = top_shap if cfg.perm_max_features is None else top_shap[:cfg.perm_max_features]\n",
    "\n",
    "        base_auc = roc_auc_score(yte, p_te)\n",
    "        rng0 = np.random.RandomState(1 + fold)\n",
    "        rsum = np.zeros(len(perm_pool), dtype=np.float32)\n",
    "        cols = list(perm_pool)\n",
    "\n",
    "        for r in range(cfg.perm_repeats):\n",
    "            Xp = Xte.copy()\n",
    "            deltas = []\n",
    "            for c in cols:\n",
    "                Xp[c] = rng0.permutation(Xp[c].values)\n",
    "                auc_p = roc_auc_score(yte, model.predict_proba(Xp)[:,1])\n",
    "                deltas.append(max(base_auc - auc_p, 0.0))\n",
    "                Xp[c] = Xte[c].values  # restore\n",
    "            ranks = pd.Series(deltas, index=cols).rank(pct=True)\n",
    "            rsum += ranks.values\n",
    "\n",
    "        perm_rank = pd.Series(rsum / max(1, cfg.perm_repeats), index=cols, name=\"perm_rank\")\n",
    "\n",
    "        gte_r = gte.rank(pct=True)\n",
    "        perm_r = perm_rank.rank(pct=True)\n",
    "        top = list(set(gte_r.sort_values(ascending=False).head(K).index)\n",
    "                   & set(perm_r.sort_values(ascending=False).head(K).index))\n",
    "        concord = spearman_rank_corr(gte_r.loc[top], perm_r.loc[top]) if top else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            \"fold\": fold,\n",
    "            \"Train_AUC\": tA[-1], \"Train_F1\": tF[-1], \"Train_Precision\": tP[-1], \"Train_Recall\": tR[-1],\n",
    "            \"AUC\": A[-1], \"F1\": F[-1], \"Precision\": P[-1], \"Recall\": R[-1],\n",
    "            \"Generalizability\": spearman_rank_corr(gtr, gte),\n",
    "            \"Concordance\": concord\n",
    "        })\n",
    "\n",
    "    perfold = pd.DataFrame(rows)\n",
    "\n",
    "    def _mean_pairwise_spearman(series_list: List[pd.Series]) -> float:\n",
    "        if len(series_list)<2: return np.nan\n",
    "        vals=[spearman_rank_corr(series_list[i], series_list[j])\n",
    "              for i in range(len(series_list)) for j in range(i+1,len(series_list))]\n",
    "        return float(np.nanmean(vals))\n",
    "    stability = _mean_pairwise_spearman(expl_te)\n",
    "\n",
    "    gap = float(np.mean(tA)) - float(np.mean(A))\n",
    "    ri = float(np.nanmean([\n",
    "        rescale01_rho(perfold[\"Generalizability\"].mean()),\n",
    "        rescale01_rho(perfold[\"Concordance\"].mean()),\n",
    "        rescale01_rho(stability)\n",
    "    ]))\n",
    "\n",
    "    summary = pd.DataFrame([{\n",
    "        \"Model\": model_name,\n",
    "        \"Train_AUC_mean\": float(np.mean(tA)), \"AUC_mean\": float(np.mean(A)),\n",
    "        \"F1_mean\": float(np.mean(F)), \"Precision_mean\": float(np.mean(P)), \"Recall_mean\": float(np.mean(R)),\n",
    "        \"Generalizability_mean\": float(perfold[\"Generalizability\"].mean()),\n",
    "        \"Concordance_mean\": float(perfold[\"Concordance\"].mean()),\n",
    "        \"Stability\": float(stability), \"ReliabilityIndex\": ri, \"OverfitGap\": gap\n",
    "    }])\n",
    "\n",
    "    mean_imp = pd.concat(expl_te, axis=1)\n",
    "    mean_imp.columns = [f\"fold{i+1}\" for i in range(mean_imp.shape[1])]\n",
    "    mean_imp = mean_imp.mean(axis=1).sort_values(ascending=False).rename(\"mean_importance_across_folds_test\")\n",
    "    return summary, {\"per_fold\": perfold, \"mean_importance_test\": mean_imp}\n",
    "\n",
    "# ===================== plotting =====================\n",
    "def plot_topk(importance: pd.Series, top_k=10, title=None):\n",
    "    if importance is None or importance.empty: return\n",
    "    imp = importance.sort_values(ascending=False).head(min(top_k, len(importance)))\n",
    "    y = np.arange(len(imp))\n",
    "    plt.figure(figsize=(9, max(4, 0.5*len(imp))))\n",
    "    plt.barh(y, imp.values); plt.yticks(y, imp.index)\n",
    "    plt.gca().invert_yaxis(); plt.xlabel(\"importance\")\n",
    "    plt.title(title or f\"Top {len(imp)} — SHAP importance\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "# ===================== run helper for notebooks =====================\n",
    "def run_experiment(*, dataset: str, model_name: str = \"random_forest\",\n",
    "                   target_col: Optional[str] = None, top_k: int = 10,\n",
    "                   cfg_overrides: Optional[Dict]=None, show_plot: bool = True):\n",
    "    cfg = PipelineConfig()\n",
    "    if cfg_overrides:\n",
    "        for k,v in cfg_overrides.items():\n",
    "            if hasattr(cfg,k): setattr(cfg,k,v)\n",
    "            else: raise KeyError(f\"Invalid override: {k}\")\n",
    "\n",
    "    set_seed(cfg.rng)\n",
    "    X,y = load_dataset(dataset, target_col=target_col)\n",
    "    df_cls, arts = run_reliability_kfold(X,y,cfg, model_name=model_name)\n",
    "\n",
    "    try:\n",
    "        from IPython.display import display; display(df_cls.round(4))\n",
    "    except Exception:\n",
    "        print(df_cls.round(4))\n",
    "    if show_plot:\n",
    "        plot_topk(arts[\"mean_importance_test\"], top_k=top_k,\n",
    "                  title=f\"Top {top_k} — {model_name.upper()} (SHAP)\")\n",
    "    return df_cls, arts\n",
    "\n",
    "# ----------------- Examples (uncomment to run) -----------------\n",
    "# df_cm1, arts_cm1 = run_experiment(dataset=\"CM1\", model_name=\"random_forest\",\n",
    "#     cfg_overrides=dict(fast_mode=True, shap_mode=\"auto\", fast_shap_k=64,\n",
    "#                        shap_eval_size=4000, shap_nsamples=200,\n",
    "#                        perm_repeats=12, concordance_top_k=20,\n",
    "#                        rf_n_estimators=800, rf_max_depth=12))\n",
    "# df_pc2, arts_pc2 = run_experiment(dataset=\"PC2\", model_name=\"extra_trees\")\n",
    "# df_lr, arts_lr   = run_experiment(dataset=\"PC1\", model_name=\"logistic_regression\",\n",
    "#     cfg_overrides=dict(shap_nsamples=200, fast_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b3bd574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uditjain/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ca05e3ddd44ceabcc27692291e50f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1894 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uditjain/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcfaaa660e9471ab366b79812a17d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uditjain/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170d63c51f5a494b9a9e0e59b44829c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/uditjain/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1db287dd7b4a6480c54b7a32c62621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1893 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 248\u001b[0m, in \u001b[0;36mTreeModel.shap_importance\u001b[0;34m(self, X_bg, X_eval)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     expl \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTreeExplainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_perturbation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtree_path_dependent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# FAST\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobability\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     sv \u001b[38;5;241m=\u001b[39m expl\u001b[38;5;241m.\u001b[39mshap_values(X_eval, check_additivity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/shap/explainers/_tree.py:206\u001b[0m, in \u001b[0;36mTreeExplainer.__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, link, linearize_link)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel_output \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly model_output=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m is supported for feature_perturbation=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mtree_path_dependent\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Only model_output=\"raw\" is supported for feature_perturbation=\"tree_path_dependent\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_mc2, arts_mc2 \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMC1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_forest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcfg_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfast_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_shap_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mshap_eval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshap_nsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mperm_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcordance_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrf_n_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrf_max_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 445\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(dataset, model_name, target_col, top_k, cfg_overrides, show_plot)\u001b[0m\n\u001b[1;32m    443\u001b[0m set_seed(cfg\u001b[38;5;241m.\u001b[39mrng)\n\u001b[1;32m    444\u001b[0m X,y \u001b[38;5;241m=\u001b[39m load_dataset(dataset, target_col\u001b[38;5;241m=\u001b[39mtarget_col)\n\u001b[0;32m--> 445\u001b[0m df_cls, arts \u001b[38;5;241m=\u001b[39m \u001b[43mrun_reliability_kfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display; display(df_cls\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 350\u001b[0m, in \u001b[0;36mrun_reliability_kfold\u001b[0;34m(X, y, cfg, model_name)\u001b[0m\n\u001b[1;32m    347\u001b[0m P\u001b[38;5;241m.\u001b[39mappend(precision_score(yte,yhat_te, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)); R\u001b[38;5;241m.\u001b[39mappend(recall_score(yte,yhat_te))\n\u001b[1;32m    349\u001b[0m \u001b[38;5;66;03m# ---- SHAP once per fold (test set only for speed)\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m gte \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXfit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXte\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m expl_te\u001b[38;5;241m.\u001b[39mappend(gte)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# ultra-cheap proxy for Generalizability in fast_mode\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 259\u001b[0m, in \u001b[0;36mTreeModel.shap_importance\u001b[0;34m(self, X_bg, X_eval)\u001b[0m\n\u001b[1;32m    257\u001b[0m     expl \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mKernelExplainer(f, bg, link\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    258\u001b[0m     ns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mshap_nsamples \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mfast_mode \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m100\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mshap_nsamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m     sv \u001b[38;5;241m=\u001b[39m \u001b[43mexpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m imp_vec \u001b[38;5;241m=\u001b[39m _mean_abs_by_feature(sv, n_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(cols))\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mSeries(imp_vec, index\u001b[38;5;241m=\u001b[39mcols, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean|SHAP|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/shap/explainers/_kernel.py:271\u001b[0m, in \u001b[0;36mKernelExplainer.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_index:\n\u001b[1;32m    270\u001b[0m     data \u001b[38;5;241m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m], index_name)\n\u001b[0;32m--> 271\u001b[0m explanations\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgc_collect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    273\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/shap/explainers/_kernel.py:482\u001b[0m, in \u001b[0;36mKernelExplainer.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m phi_var \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mgroups_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD))\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD):\n\u001b[0;32m--> 482\u001b[0m     vphi, vphi_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnsamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     phi[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds, d] \u001b[38;5;241m=\u001b[39m vphi\n\u001b[1;32m    484\u001b[0m     phi_var[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaryingInds, d] \u001b[38;5;241m=\u001b[39m vphi_var\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/shap/explainers/_kernel.py:671\u001b[0m, in \u001b[0;36mKernelExplainer.solve\u001b[0;34m(self, fraction_evaluated, dim)\u001b[0m\n\u001b[1;32m    669\u001b[0m         kwg \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    670\u001b[0m     model \u001b[38;5;241m=\u001b[39m make_pipeline(StandardScaler(with_mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), LassoLarsIC(criterion\u001b[38;5;241m=\u001b[39mc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwg))\n\u001b[0;32m--> 671\u001b[0m     nonzero_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_aug\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meyAdj_aug\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcoef_)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    673\u001b[0m \u001b[38;5;66;03m# use a fixed regularization coefficient\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m     nonzero_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(Lasso(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_reg)\u001b[38;5;241m.\u001b[39mfit(mask_aug, eyAdj_aug)\u001b[38;5;241m.\u001b[39mcoef_)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.11/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:2279\u001b[0m, in \u001b[0;36mLassoLarsIC.fit\u001b[0;34m(self, X, y, copy_X)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2276\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcriterion should be either bic or aic, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2277\u001b[0m     )\n\u001b[0;32m-> 2279\u001b[0m residuals \u001b[38;5;241m=\u001b[39m y[:, np\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_path_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2280\u001b[0m residuals_sum_squares \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(residuals\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2281\u001b[0m degrees_of_freedom \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(coef_path_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_mc2, arts_mc2 = run_experiment(dataset=\"MC1\", model_name=\"random_forest\",\n",
    "    cfg_overrides=dict(fast_mode=True, shap_mode=\"auto\", fast_shap_k=64,\n",
    "                       shap_eval_size=4000, shap_nsamples=200,\n",
    "                       perm_repeats=12, concordance_top_k=20,\n",
    "                       rf_n_estimators=800, rf_max_depth=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980e48a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
